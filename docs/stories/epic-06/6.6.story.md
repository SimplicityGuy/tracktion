# Story 6.6: Track Library Building

## Story Information
- **Epic**: 6 - Tracklist Management
- **Story Number**: 6.6
- **Status**: Deferred
- **Created**: 2025-08-27
- **Updated**: 2025-08-27
- **Dependencies**: Stories 6.1-6.2, 6.4-6.5 should be complete for full tracklist infrastructure
- **Note**: Story 6.3 (Auto Detection) has been deferred; track discovery focuses on manual/imported tracklists
- **Note**: This is the final story in Epic 6

## Story Statement
**As a** DJ discovering new music,
**I want** to identify and catalog individual tracks from mixes,
**So that** I can build my library.

## Acceptance Criteria
1. Extract track information from tracklists
2. Search for individual tracks online
3. Add to want-list or catalog
4. Track source mix information
5. Discovery statistics

## Dev Notes

### Previous Story Insights
From Stories 6.1-6.2, 6.4-6.5 implementation:
- Complete tracklist infrastructure with import and manual creation
- Individual track data available in TrackEntry model
- 1001tracklists API integration for external data
- Catalog integration patterns from Story 6.2
- CUE generation capabilities from Story 6.4
- Version history and sync capabilities from Story 6.5
[Source: Stories 6.1-6.2, 6.4-6.5 implementation]
# Note: Track discovery focuses on manually created and imported tracklists

From Epic 1 (Foundation):
- Core catalog and Recording model established
- File management and cataloging infrastructure
[Source: Epic 1 implementation]

### Project Structure Requirements
Track library building will span:
- Tracklist Service: `services/tracklist_service/src/` (track extraction)
- Cataloging Service: `services/cataloging_service/src/` (library management)
- External APIs: Music databases for track information
- Database: PostgreSQL for library data
- Redis: For caching track metadata
[Source: architecture/high-level-architecture.md]

### Data Models
**Library and Discovery Models**:
```python
class LibraryTrack:
    id: UUID
    artist: str
    title: str
    remix: Optional[str]
    label: Optional[str]
    catalog_number: Optional[str]
    release_date: Optional[date]
    genre: Optional[str]
    bpm: Optional[float]
    key: Optional[str]
    duration: Optional[timedelta]
    external_ids: Dict  # Spotify, Beatport, Discogs IDs
    created_at: datetime
    updated_at: datetime
    # Discovery tracking
    discovered_from: List[UUID]  # Tracklist IDs where discovered
    discovery_count: int  # Times found in different mixes
    first_discovered: datetime
    last_seen: datetime

class WantList:
    id: UUID
    user_id: Optional[str]  # For future multi-user support
    track_id: UUID  # Links to LibraryTrack
    priority: int  # 1-5 priority level
    status: str  # "wanted", "purchased", "unavailable"
    added_at: datetime
    notes: Optional[str]
    source_tracklist_id: UUID  # Where discovered
    purchase_links: List[Dict]  # Store URLs and prices

class TrackDiscovery:
    id: UUID
    tracklist_id: UUID
    track_position: int
    library_track_id: Optional[UUID]  # Null if not yet identified
    discovery_date: datetime
    confidence: float
    identification_method: str  # "manual", "api", "fingerprint"
    metadata_sources: List[str]  # APIs/services that provided data

class DiscoveryStatistics:
    id: UUID
    period: str  # "daily", "weekly", "monthly", "all-time"
    period_start: datetime
    period_end: datetime
    tracks_discovered: int
    unique_artists: int
    unique_labels: int
    top_genres: List[Dict]  # Genre and count
    discovery_sources: Dict  # Source type and count
    most_played_tracks: List[Dict]  # Track and play count
```
[Source: Extension of existing models for library building]

### External Music Database APIs
**Potential Integration Sources**:
```python
MUSIC_APIS = {
    "spotify": {
        "search_endpoint": "https://api.spotify.com/v1/search",
        "rate_limit": 180,  # requests per minute
        "auth": "oauth2",
        "provides": ["preview_url", "popularity", "audio_features"]
    },
    "beatport": {
        "search_endpoint": "https://api.beatport.com/v4/catalog/search",
        "rate_limit": 60,
        "auth": "api_key",
        "provides": ["price", "genre", "label", "release_date", "key", "bpm"]
    },
    "discogs": {
        "search_endpoint": "https://api.discogs.com/database/search",
        "rate_limit": 60,
        "auth": "token",
        "provides": ["label", "catalog_number", "release_year", "credits"]
    },
    "musicbrainz": {
        "search_endpoint": "https://musicbrainz.org/ws/2/recording",
        "rate_limit": 50,
        "auth": "none",
        "provides": ["isrc", "artist_credits", "release_info"]
    }
}
```
[Source: Common music database APIs]

### Track Identification Pipeline
```python
class TrackIdentificationPipeline:
    def identify_track(self, track_entry: TrackEntry) -> LibraryTrack:
        """
        Multi-stage track identification pipeline.
        """
        # Stage 1: Check if already in library
        existing = self.find_in_library(track_entry)
        if existing and existing.confidence > 0.9:
            return existing

        # Stage 2: Search external APIs
        results = []
        for api in self.enabled_apis:
            try:
                api_results = self.search_api(api, track_entry)
                results.extend(api_results)
            except RateLimitError:
                self.queue_for_retry(api, track_entry)

        # Stage 3: Fuzzy match and score results
        best_match = self.find_best_match(results, track_entry)

        # Stage 4: Enrich with additional metadata
        if best_match:
            enriched = self.enrich_metadata(best_match)

            # Stage 5: Add to library
            library_track = self.add_to_library(enriched)

            return library_track

        # Stage 6: Create provisional entry for manual review
        return self.create_provisional_track(track_entry)

    def enrich_metadata(self, track: Dict) -> Dict:
        """
        Gather additional metadata from multiple sources.
        """
        enriched = track.copy()

        # Get audio features from Spotify
        if track.get('spotify_id'):
            features = self.get_spotify_audio_features(track['spotify_id'])
            enriched.update(features)

        # Get price from Beatport
        if track.get('beatport_id'):
            price_info = self.get_beatport_price(track['beatport_id'])
            enriched['purchase_links'].append(price_info)

        # Get label info from Discogs
        if track.get('label'):
            label_info = self.get_discogs_label_info(track['label'])
            enriched['label_details'] = label_info

        return enriched
```

### Want List Management
```python
class WantListManager:
    def add_to_want_list(self, track_id: UUID, tracklist_id: UUID, priority: int = 3) -> WantList:
        """Add track to want list with source tracking."""
        # Check if already in want list
        existing = self.get_want_list_entry(track_id)
        if existing:
            # Update priority if higher
            if priority > existing.priority:
                existing.priority = priority
            return existing

        # Create new want list entry
        want_entry = WantList(
            track_id=track_id,
            priority=priority,
            status="wanted",
            source_tracklist_id=tracklist_id,
            added_at=datetime.utcnow()
        )

        # Find purchase links
        purchase_links = self.find_purchase_links(track_id)
        want_entry.purchase_links = purchase_links

        # Send notification (future feature)
        self.notify_new_want_list_item(want_entry)

        return want_entry

    def find_purchase_links(self, track_id: UUID) -> List[Dict]:
        """Find where to buy the track."""
        track = self.get_library_track(track_id)
        links = []

        # Beatport
        if track.external_ids.get('beatport'):
            links.append({
                "store": "Beatport",
                "url": f"https://www.beatport.com/track/-/{track.external_ids['beatport']}",
                "price": self.get_beatport_price(track.external_ids['beatport'])
            })

        # Bandcamp search
        bandcamp_url = self.search_bandcamp(track.artist, track.title)
        if bandcamp_url:
            links.append({
                "store": "Bandcamp",
                "url": bandcamp_url,
                "price": "Variable"
            })

        # iTunes/Apple Music
        if track.external_ids.get('itunes'):
            links.append({
                "store": "iTunes",
                "url": f"https://music.apple.com/track/{track.external_ids['itunes']}",
                "price": self.get_itunes_price(track.external_ids['itunes'])
            })

        return links
```

### Discovery Statistics Generator
```python
class DiscoveryStatsGenerator:
    def generate_statistics(self, period: str = "weekly") -> DiscoveryStatistics:
        """Generate discovery statistics for the specified period."""
        period_start, period_end = self.get_period_bounds(period)

        # Get all discoveries in period
        discoveries = self.get_discoveries(period_start, period_end)

        stats = DiscoveryStatistics(
            period=period,
            period_start=period_start,
            period_end=period_end,
            tracks_discovered=len(discoveries),
            unique_artists=self.count_unique_artists(discoveries),
            unique_labels=self.count_unique_labels(discoveries),
            top_genres=self.calculate_top_genres(discoveries),
            discovery_sources=self.aggregate_sources(discoveries),
            most_played_tracks=self.find_most_played(discoveries)
        )

        return stats

    def calculate_discovery_trends(self) -> Dict:
        """Calculate trending patterns in discoveries."""
        return {
            "trending_artists": self.get_trending_artists(days=30),
            "trending_genres": self.get_trending_genres(days=30),
            "discovery_rate": self.calculate_discovery_rate(),
            "preferred_sources": self.analyze_source_preferences(),
            "purchase_conversion": self.calculate_purchase_rate()
        }
```

### Batch Track Processing
```python
def process_tracklist_for_discovery(tracklist_id: UUID):
    """Extract and process all tracks from a tracklist."""
    tracklist = get_tracklist(tracklist_id)
    discoveries = []

    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []

        for position, track in enumerate(tracklist.tracks):
            # Submit identification task
            future = executor.submit(
                identify_and_catalog_track,
                track,
                tracklist_id,
                position
            )
            futures.append(future)

        # Collect results
        for future in as_completed(futures):
            try:
                discovery = future.result(timeout=30)
                discoveries.append(discovery)
            except Exception as e:
                log_error(f"Failed to process track: {e}")

    # Update statistics
    update_discovery_statistics(discoveries)

    # Notify about new discoveries
    notify_new_discoveries(discoveries)

    return discoveries
```

### API Specifications
**REST Endpoints to Implement**:
```
# Track Discovery
POST /api/v1/tracklists/{id}/discover-tracks
  - Body: { process_all: bool, positions: [int] }
  - Returns: List of discovered tracks

GET /api/v1/library/tracks
  - Query params: artist, title, genre, label, discovered_after
  - Returns: Paginated library tracks

GET /api/v1/library/tracks/{id}
  - Returns: Complete track information with all metadata

POST /api/v1/library/tracks/{id}/identify
  - Body: { artist: "...", title: "...", external_ids: {} }
  - Returns: Updated track with identification

# Want List Management
GET /api/v1/want-list
  - Query params: status, priority, added_after
  - Returns: Want list entries

POST /api/v1/want-list
  - Body: { track_id: "...", priority: int, notes: "..." }
  - Returns: Created want list entry

PUT /api/v1/want-list/{id}
  - Body: { status: "purchased", purchase_date: "..." }
  - Returns: Updated want list entry

DELETE /api/v1/want-list/{id}
  - Returns: Success status

# Purchase Links
GET /api/v1/library/tracks/{id}/purchase-links
  - Returns: Available purchase links with prices

POST /api/v1/library/tracks/{id}/refresh-purchase-links
  - Returns: Updated purchase links

# Discovery Statistics
GET /api/v1/discovery/statistics
  - Query params: period (daily, weekly, monthly, all-time)
  - Returns: Discovery statistics

GET /api/v1/discovery/trends
  - Returns: Trending analysis

GET /api/v1/discovery/sources/{tracklist_id}
  - Returns: Tracks discovered from specific tracklist

# Export
GET /api/v1/want-list/export
  - Query params: format (csv, json, txt)
  - Returns: Want list export file

GET /api/v1/library/export
  - Query params: format, discovered_after, genre
  - Returns: Library export file
```
[Source: Extension of Epic 6 API specifications]

### Caching Strategy
```python
CACHE_CONFIG = {
    "track_metadata": {
        "ttl": 86400,  # 24 hours
        "key_pattern": "track:{track_id}:metadata"
    },
    "api_search_results": {
        "ttl": 3600,  # 1 hour
        "key_pattern": "search:{api}:{query_hash}"
    },
    "purchase_links": {
        "ttl": 7200,  # 2 hours
        "key_pattern": "track:{track_id}:purchase"
    },
    "statistics": {
        "ttl": 900,  # 15 minutes
        "key_pattern": "stats:{period}:{date}"
    }
}
```

### Technical Constraints
- API rate limits must be respected (implement exponential backoff)
- Track identification timeout: 30 seconds per track
- Batch processing: Maximum 100 tracks per job
- Cache external API results to minimize requests
- Statistics generation: Maximum 5 seconds
- Want list size limit: 10,000 tracks per user
[Source: Practical limits and API constraints]

### File Locations
Based on project structure, new code should be created in:

**Tracklist Service**:
- Discovery API: `services/tracklist_service/src/api/discovery_endpoints.py` (new)
- Library service: `services/tracklist_service/src/services/library_service.py` (new)
- Want list service: `services/tracklist_service/src/services/want_list_service.py` (new)
- Discovery service: `services/tracklist_service/src/services/discovery_service.py` (new)
- Stats service: `services/tracklist_service/src/services/statistics_service.py` (new)
- Models: `services/tracklist_service/src/models/library.py` (new)
- External API clients: `services/tracklist_service/src/clients/` (new directory)
  - `spotify_client.py`
  - `beatport_client.py`
  - `discogs_client.py`
  - `musicbrainz_client.py`
- Tests: `tests/unit/tracklist_service/test_discovery_endpoints.py` (new)
- Tests: `tests/unit/tracklist_service/test_library_service.py` (new)

## Tasks / Subtasks

- [ ] **Task 1: Create Library Data Models** (AC: 1, 4)
  - [ ] Create `services/tracklist_service/src/models/library.py`
  - [ ] Define LibraryTrack model with all metadata fields
  - [ ] Define WantList model for tracking wanted tracks
  - [ ] Define TrackDiscovery model for discovery tracking
  - [ ] Define DiscoveryStatistics model
  - [ ] Create Alembic migration for new tables
  - [ ] Run migration with `uv run alembic upgrade head`
  - [ ] Write unit tests for models

- [ ] **Task 2: Implement External API Clients** (AC: 2, 3)
  - [ ] Create `services/tracklist_service/src/clients/` directory
  - [ ] Implement Spotify API client with OAuth2
  - [ ] Implement Beatport API client
  - [ ] Implement Discogs API client
  - [ ] Implement MusicBrainz API client
  - [ ] Add rate limiting and retry logic for each
  - [ ] Write unit tests with mocked API responses

- [ ] **Task 3: Create Track Identification Pipeline** (AC: 1, 2)
  - [ ] Create `services/tracklist_service/src/services/discovery_service.py`
  - [ ] Implement multi-stage identification pipeline
  - [ ] Add fuzzy matching for API results
  - [ ] Implement confidence scoring
  - [ ] Add metadata enrichment from multiple sources
  - [ ] Handle provisional tracks for manual review
  - [ ] Write comprehensive unit tests

- [ ] **Task 4: Implement Library Service** (AC: 1, 4)
  - [ ] Create `services/tracklist_service/src/services/library_service.py`
  - [ ] Implement track addition to library
  - [ ] Add duplicate detection and merging
  - [ ] Track discovery source information
  - [ ] Implement library search and filtering
  - [ ] Add track metadata updates
  - [ ] Write unit tests

- [ ] **Task 5: Create Want List Service** (AC: 3)
  - [ ] Create `services/tracklist_service/src/services/want_list_service.py`
  - [ ] Implement want list CRUD operations
  - [ ] Add priority management
  - [ ] Implement purchase link discovery
  - [ ] Add status tracking (wanted, purchased, unavailable)
  - [ ] Create export functionality
  - [ ] Write unit tests

- [ ] **Task 6: Implement Statistics Service** (AC: 5)
  - [ ] Create `services/tracklist_service/src/services/statistics_service.py`
  - [ ] Generate discovery statistics by period
  - [ ] Calculate trending artists and genres
  - [ ] Track discovery sources and success rates
  - [ ] Implement most-played tracks analysis
  - [ ] Add purchase conversion metrics
  - [ ] Write unit tests for statistics

- [ ] **Task 7: Create Discovery API Endpoints** (AC: 1, 2, 5)
  - [ ] Create `services/tracklist_service/src/api/discovery_endpoints.py`
  - [ ] Implement track discovery endpoint
  - [ ] Add library browsing endpoints
  - [ ] Implement statistics endpoints
  - [ ] Add trend analysis endpoint
  - [ ] Write API tests with pytest

- [ ] **Task 8: Implement Want List API** (AC: 3)
  - [ ] Add want list CRUD endpoints
  - [ ] Implement purchase link endpoints
  - [ ] Add export endpoints (CSV, JSON, TXT)
  - [ ] Implement bulk operations
  - [ ] Write API tests

- [ ] **Task 9: Add Batch Processing** (AC: 1)
  - [ ] Implement batch track discovery from tracklists
  - [ ] Add parallel processing with ThreadPoolExecutor
  - [ ] Handle API rate limits across batch jobs
  - [ ] Implement progress tracking
  - [ ] Add failure recovery
  - [ ] Write integration tests

- [ ] **Task 10: Implement Caching Layer**
  - [ ] Add Redis caching for API results
  - [ ] Cache track metadata
  - [ ] Cache purchase links with TTL
  - [ ] Implement cache invalidation
  - [ ] Add cache warming for popular tracks
  - [ ] Write cache tests

- [ ] **Task 11: Add RabbitMQ Integration**
  - [ ] Define message schemas for discovery jobs
  - [ ] Implement async discovery processing
  - [ ] Add discovery completion notifications
  - [ ] Handle batch job orchestration
  - [ ] Implement dead letter queue for failures
  - [ ] Write integration tests

- [ ] **Task 12: Run All Tests and Validation**
  - [ ] Run all unit tests: `uv run pytest tests/unit/tracklist_service/ -v`
  - [ ] Test with mock external APIs
  - [ ] Verify rate limiting works correctly
  - [ ] Test batch processing with various sizes
  - [ ] Run pre-commit hooks: `uv run pre-commit run --all-files`
  - [ ] Ensure mypy type checking passes
  - [ ] Verify code coverage meets 80% threshold

## Testing Standards
[Source: architecture/test-strategy-and-standards.md]

- **Framework**: Use `pytest` with `uv run pytest` execution
- **Test Files**: Place in `tests/unit/tracklist_service/`
- **Coverage Goal**: Minimum 80% for new code
- **API Mocking**: Mock all external API calls in tests
- **Rate Limit Tests**: Verify rate limiting and backoff work
- **Batch Tests**: Test with various batch sizes
- **Statistics Tests**: Verify calculations are accurate
- **Pre-commit**: Must pass all hooks before committing

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-27 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be populated by Dev Agent)

### Debug Log References
(To be populated by Dev Agent)

### Completion Notes
(To be populated by Dev Agent)

### File List
(To be populated by Dev Agent)

## QA Results
(To be populated by QA Agent)
