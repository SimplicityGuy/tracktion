# Story 2.3: Advanced Audio Analysis - BPM Detection

## Story Information
- **Epic**: 2 - Metadata Analysis & Naming
- **Story Number**: 2.3
- **Status**: Approved
- **Created**: 2025-08-18
- **Updated**: 2025-08-19

## Story Statement
**As a** DJ or music enthusiast,
**I want** automatic BPM (beats per minute) detection for my audio files with temporal analysis,
**so that** I can organize my music by tempo and identify tracks with tempo changes.

## Acceptance Criteria
1. BPM detection algorithm is integrated using the library selected in Story 2.2 research (likely Essentia or librosa).
2. Temporal BPM analysis is implemented:
   - BPM calculated for configurable time windows (default: 10-second intervals)
   - Start BPM (first 30 seconds) and End BPM (last 30 seconds) are captured
   - Average BPM across entire track is calculated
   - Tempo stability score indicates if BPM is constant or variable
3. The analysis service can process audio files with accuracy within ±2 BPM tolerance for constant-tempo tracks.
4. BPM values and temporal data are stored as structured metadata in both PostgreSQL and Neo4j:
   - Average BPM as primary value
   - Start/End BPM for tempo changes
   - Temporal BPM array for detailed analysis (optional storage based on configuration)
   - Confidence scores for all measurements
5. Processing is optimized to handle large files efficiently (streaming/chunking for files >100MB).
6. Caching mechanism using Redis prevents re-analysis of already processed files.
7. Performance benchmarks show processing time <30 seconds for typical 5-minute tracks.
8. Unit tests validate BPM detection accuracy using reference tracks with known BPMs.
9. Special handling for:
   - Tracks with multiple tempo changes (DJ mixes, live recordings)
   - Beatless or ambient tracks (return null or very low confidence)
   - Variable tempo music (classical, jazz)

## Dev Notes

### Previous Story Insights
From Story 2.2 research completion:
- **Recommended Library**: Essentia RhythmExtractor2013 provides 30% better accuracy than librosa
- **Installation**: Successfully installed essentia with Python 3.12 (not 3.13)
- **Performance**: Essentia processes BPM detection in ~1.77s average
- **Confidence Scores**: Essentia provides confidence values for all measurements
- **Memory Usage**: Essentia uses 47% less memory than librosa
- **Production Pattern**: Use Essentia primary with librosa fallback
- **Failed Libraries**: madmom and aubio have compilation issues on macOS
[Source: Story 2.2 Dev Agent Record]

### Technical Stack
- **Language**: Python 3.12 (required for Essentia compatibility - do NOT use 3.13)
- **Package Management**: uv (MANDATORY - never use pip or python directly)
- **ORM**: SQLAlchemy (latest)
- **Database Migrations**: Alembic (latest)
- **Databases**: PostgreSQL 17, Neo4j 5.26
- **Cache**: Redis 7.4
- **Messaging**: RabbitMQ 4.0
- **Testing**: pytest (latest) with pytest-cov for coverage
- **Linting**: ruff (latest)
- **Type Checking**: mypy (latest)
- **Containerization**: Docker (latest)
[Source: architecture/tech-stack.md]

### Service Location and Structure
The BPM detection will be added to the existing analysis_service:
- **Service Path**: `services/analysis_service/`
- **Source Code**: `services/analysis_service/src/`
- **Existing Files**:
  - `main.py` - Service entry point
  - `message_consumer.py` - RabbitMQ message handling
  - `metadata_extractor.py` - Basic metadata extraction
  - `storage_handler.py` - Database storage logic
  - `exceptions.py` - Custom exceptions
- **New Files to Create**:
  - `bpm_detector.py` - BPM detection implementation
  - `temporal_analyzer.py` - Temporal BPM analysis
  - `audio_cache.py` - Redis caching for processed files
[Source: architecture/source-tree.md]

### Data Models and Storage

#### PostgreSQL Metadata Storage
BPM data will be stored in the existing `metadata` table:
```sql
-- Existing table structure
CREATE TABLE metadata (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    recording_id UUID REFERENCES recordings(id),
    key VARCHAR(255) NOT NULL,
    value TEXT NOT NULL
);
```
BPM metadata keys:
- `bpm_average` - Average BPM across entire track
- `bpm_start` - BPM for first 30 seconds
- `bpm_end` - BPM for last 30 seconds
- `bpm_confidence` - Overall confidence score
- `bpm_stability` - Tempo stability score
- `bpm_temporal` - JSON array of windowed BPM values (optional)
[Source: architecture/database-schema-refined-and-finalized.md]

#### Neo4j Graph Storage
```text
(Recording:Recording { uuid: '...' })-[:HAS_METADATA]->(BPMData:Metadata {
    key: 'bpm_average',
    value: '128',
    confidence: 0.95
})
```
[Source: architecture/database-schema-refined-and-finalized.md]

### Coding Standards
- Maximum line length: 120 characters
- All Python execution via uv: `uv run python`, `uv run pytest`, etc.
- Type hints required for all function signatures
- Docstrings for all public methods
- Pre-commit hooks MUST pass before commit
- Minimum 80% test coverage for new code
[Source: architecture/coding-standards.md]

### Implementation from Research
Based on Story 2.2 research, use this production-ready pattern:
```python
def detect_bpm_with_confidence(audio_path):
    """Production-ready BPM detection"""
    import essentia.standard as es

    extractor = es.RhythmExtractor2013()
    audio = es.MonoLoader(filename=audio_path)()
    bpm, beats, confidence, _, intervals = extractor(audio)

    # Apply confidence threshold
    if confidence < 0.7:
        # Try alternative algorithm
        bpm_alt = es.PercivalBpmEstimator()(audio)
        if abs(bpm - bpm_alt) < 5:
            confidence = 0.9  # High agreement increases confidence

    return {
        'bpm': round(bpm, 1),
        'confidence': confidence,
        'needs_review': confidence < 0.8
    }
```
[Source: research/audio_analysis/reports/technical_report.md]

### Message Queue Integration
The service consumes messages from RabbitMQ:
- **Queue**: `analysis_queue` (existing)
- **Message Format**: JSON with `file_path` and `recording_id`
- **Response**: Publish results to `metadata_queue`
[Source: Previous story implementation patterns]

### Redis Caching Strategy
- **Cache Key Pattern**: `bpm:{file_hash}:{version}` where:
  - file_hash is xxh128 (fast) or sha256 (secure)
  - version tracks algorithm version for cache invalidation
- **Cache Value Structure**:
  ```json
  {
    "bpm_average": 128.5,
    "bpm_start": 126.0,
    "bpm_end": 130.0,
    "confidence": 0.95,
    "stability": 0.88,
    "temporal_data": [...],  // optional
    "algorithm_version": "1.0",
    "processed_at": "2024-01-01T00:00:00Z"
  }
  ```
- **TTL Configuration**:
  - Default: 30 days for successful analysis
  - Failed analysis: 1 hour (to allow retry)
  - Low confidence (<0.5): 7 days (for potential re-analysis)
- **Cache Invalidation Triggers**:
  - Algorithm version change
  - Manual cache flush command
  - File modification detected (different hash)
- **Cache Warming**: Pre-populate cache during off-peak hours
[Source: architecture/tech-stack.md - Redis usage]

### Performance Requirements
- Processing time: <30 seconds for 5-minute tracks
- Memory usage: <500MB per worker
- Parallel processing: Support multiple workers
- Large file handling: Stream processing for files >100MB
[Source: Epic requirements and Story 2.2 benchmarks]

## Tasks / Subtasks

### 1. Set Up BPM Detection Module
- [x] Create `services/analysis_service/src/bpm_detector.py`
- [x] Implement `BPMDetector` class with Essentia RhythmExtractor2013
- [x] Add fallback to Essentia PercivalBpmEstimator for low confidence
- [x] Include error handling for audio loading failures
- [x] Add type hints and comprehensive docstrings
- [x] Create unit test `tests/unit/analysis_service/test_bpm_detector.py`
- [x] Test with reference tracks of known BPMs (±2 BPM accuracy)
- [x] Run pre-commit hooks and fix any issues

### 2. Implement Temporal BPM Analysis
- [x] Create `services/analysis_service/src/temporal_analyzer.py`
- [x] Implement windowed BPM calculation (default 10-second windows)
- [x] Calculate start BPM (first 30 seconds) and end BPM (last 30 seconds)
- [x] Compute average BPM and tempo stability score
- [x] Handle variable tempo detection and confidence scoring
- [x] Create unit test `tests/unit/analysis_service/test_temporal_analyzer.py`
- [x] Test with DJ mixes and classical music (variable tempo)
- [x] Verify memory efficiency for long tracks

### 3. Add Redis Caching Layer
- [ ] Create `services/analysis_service/src/audio_cache.py`
- [ ] Implement Redis connection management
- [ ] Create cache key generation using file hash
- [ ] Implement cache check before processing
- [ ] Store BPM results with TTL (30 days default)
- [ ] Add cache invalidation methods
- [ ] Create unit test `tests/unit/analysis_service/test_audio_cache.py`
- [ ] Mock Redis for unit tests

### 4. Integrate BPM Detection with Message Consumer
- [ ] Update `services/analysis_service/src/message_consumer.py`
- [ ] Add BPM detection to message processing pipeline
- [ ] Check Redis cache before processing
- [ ] Handle special cases (beatless, corrupted files)
- [ ] Implement confidence threshold checking
- [ ] Update integration tests
- [ ] Test end-to-end flow with RabbitMQ

### 5. Update Storage Handler for BPM Data
- [ ] Modify `services/analysis_service/src/storage_handler.py`
- [ ] Add methods to store BPM metadata in PostgreSQL
- [ ] Store average, start, end BPM values
- [ ] Store confidence and stability scores
- [ ] Optionally store temporal BPM array (based on config)
- [ ] Add Neo4j graph storage for BPM relationships
- [ ] Create unit tests for storage operations
- [ ] Verify data persistence in both databases

### 6. Add Configuration Management
- [ ] Update service configuration for BPM settings
- [ ] Add temporal window size configuration (default 10s)
- [ ] Add confidence threshold settings
- [ ] Configure Redis connection parameters
- [ ] Add feature flags for temporal storage
- [ ] Document all configuration options
- [ ] Test configuration loading

### 7. Implement Performance Optimizations
- [ ] Add streaming/chunking for large files (>100MB)
- [ ] Implement parallel processing support
- [ ] Optimize memory usage for long tracks
- [ ] Add performance metrics logging
- [ ] Create benchmark tests
- [ ] Verify <30 second processing for 5-minute tracks
- [ ] Document performance characteristics

### 8. Create Reference Test Suite
- [ ] Gather or create test audio files with known BPMs
- [ ] Include constant tempo tracks (electronic, rock)
- [ ] Include variable tempo tracks (classical, jazz)
- [ ] Include edge cases (beatless ambient, silence)
- [ ] Create test data fixtures
- [ ] Write comprehensive integration tests
- [ ] Document test coverage and accuracy metrics

### 9. Update Documentation
- [ ] Update service README with BPM detection features
- [ ] Document API changes and message formats
- [ ] Add configuration examples
- [ ] Create troubleshooting guide
- [ ] Document performance benchmarks
- [ ] Update architecture diagrams if needed

### 10. Final Integration and Testing
- [ ] Run full unit test suite: `uv run pytest tests/unit/analysis_service/ -v`
- [ ] Run integration tests with all services running
- [ ] Verify Redis caching behavior
- [ ] Test with real audio files of various formats
- [ ] Ensure all pre-commit hooks pass
- [ ] Check code coverage meets 80% minimum
- [ ] Perform end-to-end testing of complete pipeline
- [ ] Update story status to "Done" upon completion

## Testing

### Testing Standards
- **Test Location**: `tests/unit/analysis_service/`
- **Test Files**:
  - `test_bpm_detector.py` - Core BPM detection tests
  - `test_temporal_analyzer.py` - Temporal analysis tests
  - `test_audio_cache.py` - Redis caching tests
- **Test Execution**: `uv run pytest tests/unit/analysis_service/ -v`
- **Coverage Requirement**: Minimum 80% code coverage
- **Framework**: pytest with pytest-cov for coverage reporting
[Source: architecture/test-strategy-and-standards.md]

### Required Test Coverage
1. **Accurate BPM Detection**:
   - Reference tracks with known BPMs (±2 BPM tolerance)
   - Test data: Electronic (128 BPM), Rock (120 BPM), Hip-Hop (85 BPM)
2. **Edge Cases**:
   - Beatless/ambient tracks (should return null or low confidence)
   - Variable tempo tracks (classical, jazz)
   - DJ mixes with tempo transitions
3. **Error Handling**:
   - Corrupted audio files
   - Unsupported formats
   - Network/service failures
4. **Redis Caching**:
   - Cache hit/miss behavior
   - TTL expiration
   - Cache invalidation

### Test Data Requirements
- **Reference Audio Files**:
  - `test_128bpm_electronic.mp3` - Constant 128 BPM
  - `test_120bpm_rock.mp3` - Constant 120 BPM
  - `test_variable_classical.mp3` - Variable tempo
  - `test_beatless_ambient.mp3` - No clear beat
  - `test_dj_mix.mp3` - Multiple tempo changes
- **File Formats**: MP3, FLAC, WAV, M4A
- **File Sizes**: Include files >100MB for streaming tests

## Implementation Guidance

### Critical Path
1. Start with BPM detection module using Essentia (Task 1)
2. Add temporal analysis for DJ use cases (Task 2)
3. Implement caching to avoid re-processing (Task 3)
4. Integrate with existing message pipeline (Task 4)
5. Ensure proper storage in both databases (Task 5)

### Common Pitfalls to Avoid
1. Not handling Essentia installation issues - use Python 3.12 if needed
2. Forgetting to check Redis cache before processing
3. Not handling edge cases (beatless, corrupted files)
4. Storing large temporal arrays without configuration control
5. Not validating BPM accuracy with reference tracks

### Dependencies
- Story 2.1 (Analysis Service Setup) - Complete
- Story 2.2 (Research Spike) - Complete with Essentia recommendation
- Essentia library must be installed in Docker container
- Redis service must be running for caching

## Success Metrics
- BPM detection accuracy within ±2 BPM for constant tempo tracks
- Processing time <30 seconds for 5-minute tracks
- 100% of processed files cached in Redis
- All unit tests passing with >80% coverage
- Zero memory leaks during long-running operations
- Successful handling of all audio format edge cases

## Dev Agent Record

### Agent Model Used
[To be filled by implementing agent]

### Implementation Started
- Date: [To be filled]
- Agent/Developer: [To be filled]

### Debug Log References
[To be filled during implementation]

### Completion Notes
[To be filled upon completion]

### File List
- services/analysis_service/src/bpm_detector.py (created)
- services/analysis_service/src/temporal_analyzer.py (created)
- tests/unit/analysis_service/test_bpm_detector.py (created)
- tests/unit/analysis_service/test_temporal_analyzer.py (created)

### Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-18 | 1.0 | Initial story creation from Epic 2 requirements | BMad System |
| 2025-08-19 | 1.1 | Fixed Python version to 3.12, added Testing section, added QA placeholder | Sarah (PO) |

## QA Results
[To be filled by QA Agent after implementation review]

### Test Execution Results
- [ ] Unit tests passing
- [ ] Integration tests passing
- [ ] Code coverage meets requirements
- [ ] Performance benchmarks validated

### Quality Metrics
- [ ] Code adheres to standards
- [ ] Documentation complete
- [ ] Error handling comprehensive
- [ ] Security considerations addressed

### Sign-off
- [ ] QA Agent approval
- [ ] Product Owner approval
