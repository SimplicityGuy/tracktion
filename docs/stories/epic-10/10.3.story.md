# Story 10.3: Complete Analysis Service API Implementation

## Status
Approved

## Story
**As a** system processing audio files
**I want** fully functional API endpoints
**So that** all operations work with real data instead of mocks

## Acceptance Criteria
1. All 20 "In real implementation" comments replaced with working code
2. Database operations implemented for recordings, metadata, analysis
3. Message queue submissions working for all endpoints
4. File streaming from actual storage locations
5. Proper error handling for all operations
6. Integration tests for all endpoints

## Tasks / Subtasks
- [ ] Replace mock data in recordings.py endpoints (AC: 1, 2, 3)
  - [ ] Implement real database fetch for GET /recordings
  - [ ] Replace mock data with actual recording queries
  - [ ] Add message queue submission for processing requests
  - [ ] Implement pagination and filtering for large datasets
- [ ] Complete metadata.py endpoint implementations (AC: 1, 2, 3)
  - [ ] Implement database operations for metadata extraction
  - [ ] Connect extraction to message queue for processing
  - [ ] Add enrichment workflow integration
  - [ ] Replace all mock metadata returns with real queries
- [ ] Finish analysis.py endpoint implementations (AC: 1, 2, 3)
  - [ ] Implement processing queue submissions for analysis requests
  - [ ] Connect to database for analysis result storage and retrieval
  - [ ] Add real-time analysis status tracking
  - [ ] Replace mock analysis data with actual computations
- [ ] Complete tracklist.py endpoint implementations (AC: 1, 2, 3)
  - [ ] Implement database operations for tracklist management
  - [ ] Integrate CUE parser for real tracklist processing
  - [ ] Connect processing queue for tracklist generation
  - [ ] Replace mock tracklist data with real parsing results
- [ ] Implement streaming.py file operations (AC: 4)
  - [ ] Replace mock file reading with actual file streaming
  - [ ] Implement database file path retrieval
  - [ ] Add file existence validation and error handling
  - [ ] Implement range request support for audio streaming
- [ ] Add comprehensive error handling (AC: 5)
  - [ ] Implement proper exception handling for all database operations
  - [ ] Add file not found and permission error handling
  - [ ] Create standardized error response formats
  - [ ] Add logging for all error conditions
- [ ] Implement database integration (AC: 2)
  - [ ] Create database connection and session management
  - [ ] Implement SQLAlchemy models for all data types
  - [ ] Add database query optimizations and indexing
  - [ ] Create repository pattern for data access
- [ ] Set up message queue integration (AC: 3)
  - [ ] Implement RabbitMQ producers for all processing requests
  - [ ] Create message schemas for different request types
  - [ ] Add message validation and error handling
  - [ ] Implement request tracking and correlation IDs
- [ ] Write comprehensive integration tests (AC: 6)
  - [ ] Test all endpoints with real database data
  - [ ] Test message queue integration and processing
  - [ ] Test file streaming and range requests
  - [ ] Test error handling and edge cases
  - [ ] Add performance tests for high-load scenarios

## Dev Notes

### Previous Story Insights
Story 10.3 addresses one of the core issues in Epic 10 - the analysis_service has 20 endpoints that return mock data instead of real implementations. This story transforms those placeholders into fully functional API endpoints integrated with the database and message queue systems.

### Current Analysis Service Issues
[Source: Epic 10 detailed analysis]

**20 "In real implementation" occurrences found across these files:**
- **recordings.py**: Submit to message queue, fetch from database, query operations
- **metadata.py**: Database operations, extraction message queue, enrichment workflow
- **analysis.py**: Processing queue submissions, database fetches
- **tracklist.py**: Database operations, CUE parser integration, processing queue
- **streaming.py**: Actual file reading, database file path retrieval

### Database Integration Requirements
[Source: architecture/data-models-refined-and-finalized.md, architecture/database-schema-refined-and-finalized.md]

Required database models for analysis service:
```python
# Recording model (existing schema)
class Recording:
    id: UUID
    file_path: str
    file_name: str
    sha256_hash: str
    xxh128_hash: str
    created_at: datetime

# Metadata model (existing schema)
class Metadata:
    id: UUID
    recording_id: UUID
    key: str  # bpm, mood, genre, etc.
    value: str

# Tracklist model (existing schema)
class Tracklist:
    id: UUID
    recording_id: UUID
    source: str  # manual, 1001tracklists.com, etc.
    tracks: JSONB  # Array of track objects
    cue_file_path: str

# Analysis results (need to create)
class AnalysisResult:
    id: UUID
    recording_id: UUID
    analysis_type: str  # bpm_detection, key_detection, etc.
    result_data: JSONB
    confidence_score: float
    created_at: datetime
    status: str  # pending, completed, failed
```

### Message Queue Integration
[Source: architecture/tech-stack.md, architecture/coding-standards.md]

RabbitMQ message schemas for processing requests:
```python
# Recording processing request
{
    "message_type": "recording_process",
    "recording_id": "uuid",
    "file_path": "path/to/file",
    "requested_analyses": ["bpm", "key", "mood"],
    "priority": "normal",
    "correlation_id": "uuid"
}

# Metadata extraction request
{
    "message_type": "metadata_extract",
    "recording_id": "uuid",
    "extraction_types": ["id3_tags", "audio_analysis"],
    "correlation_id": "uuid"
}

# Tracklist generation request
{
    "message_type": "tracklist_generate",
    "recording_id": "uuid",
    "source_hint": "1001tracklists",
    "correlation_id": "uuid"
}
```

### File Streaming Implementation
Real file streaming to replace mock implementations:

```python
# streaming.py - Real implementation needed
async def stream_audio_file(recording_id: UUID, range_header: str = None):
    # Replace "In real implementation, read from actual file"
    recording = await recording_repository.get_by_id(recording_id)
    if not recording:
        raise HTTPException(404, "Recording not found")

    file_path = recording.file_path
    if not os.path.exists(file_path):
        raise HTTPException(404, "File not found on disk")

    # Implement range request support for audio streaming
    return StreamingResponse(
        file_stream_generator(file_path, range_header),
        media_type="audio/mpeg",
        headers={"Accept-Ranges": "bytes"}
    )
```

### API Endpoint Transformations
Transform each placeholder into real implementation:

**recordings.py endpoints:**
```python
# BEFORE: Mock implementation
@router.get("/recordings")
async def list_recordings():
    # In real implementation, fetch from database
    return {"recordings": [mock_data]}

# AFTER: Real implementation
@router.get("/recordings")
async def list_recordings(
    page: int = 1,
    limit: int = 20,
    filter_by: str = None
):
    recordings = await recording_repository.list_paginated(
        page=page, limit=limit, filter_by=filter_by
    )
    return {"recordings": recordings, "total": recordings.total_count}
```

**metadata.py endpoints:**
```python
# BEFORE: Mock implementation
@router.post("/metadata/extract")
async def extract_metadata(recording_id: UUID):
    # In real implementation, submit to message queue for processing
    return {"status": "queued"}

# AFTER: Real implementation
@router.post("/metadata/extract")
async def extract_metadata(recording_id: UUID):
    # Validate recording exists
    recording = await recording_repository.get_by_id(recording_id)
    if not recording:
        raise HTTPException(404, "Recording not found")

    # Submit to processing queue
    correlation_id = str(uuid4())
    await message_queue.publish("metadata.extract", {
        "recording_id": str(recording_id),
        "correlation_id": correlation_id
    })

    return {"status": "queued", "correlation_id": correlation_id}
```

### Error Handling Standards
Comprehensive error handling for all operations:

```python
class AnalysisServiceError(Exception):
    pass

class RecordingNotFoundError(AnalysisServiceError):
    pass

class FileAccessError(AnalysisServiceError):
    pass

class DatabaseError(AnalysisServiceError):
    pass

class MessageQueueError(AnalysisServiceError):
    pass

# Standardized error responses
@app.exception_handler(RecordingNotFoundError)
async def recording_not_found_handler(request, exc):
    return JSONResponse(
        status_code=404,
        content={"error": "recording_not_found", "detail": str(exc)}
    )
```

### File Locations
[Source: architecture/source-tree.md]

Analysis service structure (existing):
```
services/analysis_service/
├── src/
│   ├── api/
│   │   ├── recordings.py       # Needs 4-5 endpoint implementations
│   │   ├── metadata.py         # Needs 3-4 endpoint implementations
│   │   ├── analysis.py         # Needs 5-6 endpoint implementations
│   │   ├── tracklist.py        # Needs 4-5 endpoint implementations
│   │   └── streaming.py        # Needs 2-3 endpoint implementations
│   ├── models/                 # Add analysis result models
│   ├── repositories/           # Create repository layer
│   ├── services/               # Business logic layer
│   └── utils/                  # Utility functions
├── tests/unit/
├── tests/integration/          # New integration tests needed
└── pyproject.toml
```

### Repository Pattern Implementation
Create repository layer for database operations:

```python
class RecordingRepository:
    async def get_by_id(self, recording_id: UUID) -> Optional[Recording]
    async def list_paginated(self, page: int, limit: int, **filters) -> PaginatedResult
    async def create(self, recording_data: RecordingCreate) -> Recording
    async def update(self, recording_id: UUID, updates: RecordingUpdate) -> Recording
    async def delete(self, recording_id: UUID) -> bool

class MetadataRepository:
    async def get_by_recording_id(self, recording_id: UUID) -> List[Metadata]
    async def create_batch(self, metadata_items: List[MetadataCreate]) -> List[Metadata]
    async def update_by_key(self, recording_id: UUID, key: str, value: str) -> Metadata

class TracklistRepository:
    async def get_by_recording_id(self, recording_id: UUID) -> Optional[Tracklist]
    async def create(self, tracklist_data: TracklistCreate) -> Tracklist
    async def update_tracks(self, tracklist_id: UUID, tracks: List[dict]) -> Tracklist
```

### Message Queue Integration Details
[Source: architecture/coding-standards.md - RabbitMQ for inter-service communication]

Required message producers:
```python
class MessageQueueService:
    async def publish_recording_analysis(self, recording_id: UUID, analysis_types: List[str])
    async def publish_metadata_extraction(self, recording_id: UUID, extraction_types: List[str])
    async def publish_tracklist_generation(self, recording_id: UUID, source_hint: str)
    async def publish_file_processing(self, recording_id: UUID, processing_type: str)

# Message routing
exchanges = {
    "recording.analysis": "recording.analysis.queue",
    "metadata.extraction": "metadata.extraction.queue",
    "tracklist.generation": "tracklist.generation.queue",
    "file.processing": "file.processing.queue"
}
```

### Technical Constraints
[Source: architecture/coding-standards.md, architecture/tech-stack.md]

- Use `uv run` for all Python commands
- SQLAlchemy ORM for all database operations
- No direct database queries outside repository layer
- RabbitMQ for all inter-service communication
- Structured logging for all operations
- Environment variables for configuration
- All pre-commit hooks must pass

### Performance Requirements
Analysis service performance targets:
- Database queries: <100ms for simple lookups
- Message queue publishing: <50ms per message
- File streaming: Support range requests efficiently
- API response times: <200ms for data retrieval
- Concurrent request handling: 100+ requests per second

### Integration Testing Strategy
Comprehensive testing of real implementations:

```python
# Integration test examples
class TestAnalysisServiceIntegration:
    async def test_recording_lifecycle_end_to_end(self):
        # Create recording -> Extract metadata -> Generate tracklist -> Stream file

    async def test_message_queue_integration(self):
        # Verify messages published to correct queues with proper schemas

    async def test_database_operations_under_load(self):
        # Test concurrent database operations and connection pooling

    async def test_file_streaming_with_range_requests(self):
        # Test partial content delivery for audio streaming
```

## Testing
[Source: architecture/test-strategy-and-standards.md]

### Testing Requirements
- Test location: `tests/unit/analysis_service/` and `tests/integration/analysis_service/`
- Use pytest as testing framework
- Execute with `uv run pytest tests/unit/analysis_service/ -v`
- Minimum 80% code coverage for new implementation code

### Testing Categories
1. **Repository Tests**: Database operations, connection handling, error cases
2. **API Endpoint Tests**: Request/response validation, error handling, edge cases
3. **Message Queue Tests**: Message publishing, schema validation, error handling
4. **File Streaming Tests**: Range requests, file access, error conditions
5. **Integration Tests**: End-to-end API workflows with real database
6. **Performance Tests**: Load testing with realistic data volumes
7. **Error Handling Tests**: Database failures, file access errors, queue failures

### Mock and Test Data
- Real database test data (not mocks) for integration tests
- Mock external service dependencies (file system, message queue)
- Test audio files for streaming validation
- Various edge case scenarios (missing files, corrupt data)
- Load testing data sets for performance validation

### Database Testing
- Use containerized PostgreSQL for integration tests
- Test with realistic data volumes (1000+ recordings)
- Validate database connection pooling under load
- Test transaction handling and rollback scenarios
- Verify query performance and optimization

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-01 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-09-01 | 1.1 | Renumbered from 10.5 to 10.3 in Epic reorganization | Development Team |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here after implementation*
