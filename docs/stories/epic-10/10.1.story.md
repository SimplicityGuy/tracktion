# Story 10.1: Implement Cataloging Service

## Status
Approved

## Story
**As a** system managing music files
**I want** a dedicated cataloging service
**So that** all recordings are properly tracked and managed

## Acceptance Criteria
1. Service structure created following project standards
2. Database operations implemented (CRUD for recordings)
3. RabbitMQ consumer for catalog events
4. API endpoints for catalog queries
5. Proper error handling and logging
6. Docker container configured
7. Unit tests with 80% coverage

## Tasks / Subtasks
- [ ] Create service structure (AC: 1)
  - [ ] Create service directory structure following monorepo pattern
  - [ ] Set up pyproject.toml with dependencies, following pattern of existing pyproject.toml files
  - [ ] Create main.py entry point
  - [ ] Set up logging configuration
- [ ] Implement database models (AC: 2)
  - [ ] Create SQLAlchemy models for Recording, Metadata, Tracklist
  - [ ] Set up Alembic for database migrations
  - [ ] Create database connection and session management
  - [ ] Implement repository pattern for data access
- [ ] Create database operations (AC: 2)
  - [ ] Implement CRUD operations for Recording
  - [ ] Implement CRUD operations for Metadata
  - [ ] Implement CRUD operations for Tracklist
  - [ ] Add proper transaction management
- [ ] Set up RabbitMQ integration (AC: 3)
  - [ ] Create RabbitMQ consumer for catalog events
  - [ ] Implement message handlers for different event types
  - [ ] Add message validation and error handling
  - [ ] Configure retry logic for failed messages
- [ ] Create API endpoints (AC: 4)
  - [ ] Implement GET endpoints for recordings, metadata, tracklists
  - [ ] Add search and filter capabilities
  - [ ] Implement pagination for large result sets
  - [ ] Add API documentation and validation
- [ ] Add error handling and logging (AC: 5)
  - [ ] Implement structured logging throughout service
  - [ ] Add proper exception handling for all operations
  - [ ] Create error response standards
  - [ ] Add monitoring and health check endpoints
- [ ] Configure Docker container (AC: 6)
  - [ ] Create Dockerfile following project standards
  - [ ] Add environment variable configuration
  - [ ] Configure service for docker-compose
  - [ ] Test containerized deployment
- [ ] Write comprehensive tests (AC: 7)
  - [ ] Create unit tests for all models
  - [ ] Test all repository operations
  - [ ] Test API endpoints with various scenarios
  - [ ] Test RabbitMQ message handling
  - [ ] Achieve 80% code coverage

## Dev Notes

### Previous Story Insights
This is the first story for Epic 10. The cataloging service is foundational for the system and was designed but never implemented. This service is critical for recording management and interfaces with PostgreSQL for storage.

### Data Models
[Source: architecture/data-models-refined-and-finalized.md]

Recording Model:
```python
class Recording(Base):
    id: UUID (Primary Key)
    file_path: str
    file_name: str
    sha256_hash: str (optional, unique)
    xxh128_hash: str (optional, unique)
    created_at: datetime
```

Metadata Model:
```python
class Metadata(Base):
    id: UUID (Primary Key)
    recording_id: UUID (Foreign Key to Recording)
    key: str
    value: str
```

Tracklist Model:
```python
class Tracklist(Base):
    id: UUID (Primary Key)
    recording_id: UUID (Foreign Key to Recording)
    source: str
    cue_file_path: str (optional)
    tracks: JSONB (Array of track objects)
```

### Database Schema
[Source: architecture/database-schema-refined-and-finalized.md]

PostgreSQL tables already defined:
- recordings table with UUID primary key, file_path, file_name, hash fields
- metadata table with key-value pairs linked to recordings
- tracklists table with JSONB tracks field
- Proper indexes on metadata table for performance

### API Specifications
Cataloging service endpoints needed:
- GET `/recordings` - List recordings with pagination and filters
- GET `/recordings/{id}` - Get specific recording details
- POST `/recordings/search` - Advanced search with filters
- GET `/recordings/{id}/metadata` - Get metadata for recording
- GET `/recordings/{id}/tracklist` - Get tracklist for recording
- GET `/health` - Health check endpoint
- GET `/metrics` - Service metrics endpoint

### Component Specifications
Not applicable - backend service only

### File Locations
[Source: architecture/source-tree.md]
- Service root: `services/cataloging_service/`
- Source code: `services/cataloging_service/src/`
- Models: `services/cataloging_service/src/models/`
- Repositories: `services/cataloging_service/src/repositories/`
- API: `services/cataloging_service/src/api/`
- Consumers: `services/cataloging_service/src/consumers/`
- Utils: `services/cataloging_service/src/utils/`
- Tests: `tests/unit/cataloging_service/`
- Dockerfile: `services/cataloging_service/Dockerfile`
- Config: `services/cataloging_service/pyproject.toml`

### Technical Constraints
[Source: architecture/tech-stack.md]
- Python 3.13 as primary language
- PostgreSQL 17 for database
- SQLAlchemy ORM for database operations
- Alembic for database migrations
- RabbitMQ 4.0 for message queue
- Docker for containerization
- Use uv for all Python package management

[Source: architecture/coding-standards.md]
- MUST use `uv run` for all Python commands
- Use ruff for linting and formatting
- Use mypy for type checking
- All pre-commit hooks MUST pass
- Structured logging throughout
- Environment variables for all configuration
- No direct database queries outside ORM/repository layer
- Inter-service communication only via RabbitMQ

### Message Queue Integration
RabbitMQ consumer for catalog events:
- Listen for file_discovered events
- Listen for metadata_extracted events
- Listen for tracklist_generated events
- Process events and update database accordingly
- Implement retry logic for failed message processing

### Service Configuration
Required environment variables:
- DATABASE_URL - PostgreSQL connection string
- RABBITMQ_URL - RabbitMQ connection string
- LOG_LEVEL - Logging level configuration
- SERVICE_PORT - Port for HTTP API
- SERVICE_HOST - Host binding for service

## Testing
[Source: architecture/test-strategy-and-standards.md]

### Testing Requirements
- Test location: `tests/unit/cataloging_service/`
- Use pytest as testing framework
- Execute with `uv run pytest tests/unit/cataloging_service/ -v`
- Minimum 80% code coverage for new code
- All tests must pass before story completion

### Testing Standards
- Unit tests for all models and their relationships
- Repository tests with database operations
- API endpoint tests with various scenarios
- Message consumer tests with mock messages
- Error handling tests for failure scenarios
- Performance tests for database operations

### Test Categories
1. **Model Tests**: Test SQLAlchemy models and relationships
2. **Repository Tests**: Test CRUD operations and business logic
3. **API Tests**: Test HTTP endpoints with different inputs
4. **Consumer Tests**: Test RabbitMQ message processing
5. **Integration Tests**: Test service interaction patterns

### Testing Execution
- Run after each task completion
- All tests must pass before marking story as Done
- Pre-commit hooks must pass before any commits
- Use dockerized test database for consistency

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-01 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-09-01 | 1.1 | Updated as part of Epic 10 reorganization | Development Team |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here after implementation*
