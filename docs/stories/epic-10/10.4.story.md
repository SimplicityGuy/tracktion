# Story 10.4: Implement Missing TODO Items

## Status
Approved

## Story
**As a** development team
**I want** all TODO items completed
**So that** the system has no incomplete features

## Acceptance Criteria
1. Metadata repository integrated with file rename proposal
2. Job tracking with proper UUID generation
3. Audio duration detection from actual files
4. Authentication implemented in parser admin
5. Operation history storage working
6. Version mismatch in sync_event_consumer resolved
7. All validation and conversion logic implemented

## Tasks / Subtasks
- [x] Implement file rename proposal metadata integration (AC: 1)
  - [x] Find all 5 occurrences of file rename proposal TODOs
  - [x] Integrate metadata repository with rename suggestions
  - [x] Add metadata-driven rename logic
  - [x] Test metadata integration with file naming
- [x] Implement proper job tracking system (AC: 2)
  - [x] Create job tracking data models with UUID generation
  - [x] Replace placeholder job tracking with real implementation
  - [x] Add job status updates and persistence
  - [x] Implement job correlation across services
- [ ] Add real audio duration detection (AC: 3)
  - [ ] Find 2 occurrences of audio duration detection TODOs
  - [ ] Implement actual audio file analysis for duration
  - [ ] Replace placeholder duration calculations
  - [ ] Add audio format support and validation
- [ ] Implement parser admin authentication (AC: 4)
  - [ ] Add authentication system to parser admin interface
  - [ ] Implement user management and access control
  - [ ] Secure admin endpoints with proper authorization
  - [ ] Add session management and logout functionality
- [ ] Build operation history storage system (AC: 5)
  - [ ] Create operation history data models
  - [ ] Implement history logging for all operations
  - [ ] Add history query and reporting capabilities
  - [ ] Integrate history storage across all services
- [ ] Fix version mismatch in sync_event_consumer (AC: 6)
  - [ ] Identify and resolve version compatibility issues
  - [ ] Update dependencies to compatible versions
  - [ ] Test sync event processing after version fixes
  - [ ] Add version compatibility checks
- [ ] Complete validation and conversion logic (AC: 7)
  - [ ] Implement CUE generation handler validation
  - [ ] Add audio service integration in matching service
  - [ ] Complete file retrieval from storage systems
  - [ ] Add CUE content generation and caching
  - [ ] Implement metadata extraction using CUE parser
- [ ] Add notification system integration (AC: 1-7)
  - [ ] Complete notification sending via RabbitMQ in main.py
  - [ ] Integrate webhook implementations in alert manager
  - [ ] Add search implementation with filters in cataloging service
  - [ ] Implement FileLifecycleService for cleanup
- [ ] Write comprehensive tests for all implementations (AC: 1-7)
  - [ ] Test metadata integration and file rename proposals
  - [ ] Test job tracking with UUID generation and correlation
  - [ ] Test audio duration detection with real files
  - [ ] Test authentication and authorization systems
  - [ ] Test operation history logging and queries
  - [ ] Test version compatibility and sync processing
  - [ ] Test validation, conversion, and CUE generation

## Dev Notes

### Previous Story Insights
Story 10.4 addresses the 28 TODO items identified in Epic 10 analysis. These represent incomplete features scattered throughout the codebase that need proper implementation. Unlike Story 10.3 which focused on API endpoints, this story addresses business logic, integrations, and system features that were stubbed out.

### TODO Items Analysis
[Source: Epic 10 detailed analysis - 28 TODO items requiring implementation]

**File Rename Proposal Integration (5 occurrences)**
Location: Metadata repository integration needed across multiple services
Purpose: Connect metadata analysis to intelligent file naming suggestions
Implementation: Integration with ML-driven rename proposals using metadata context

**Notification Sending via RabbitMQ in main.py**
Location: Main service orchestration
Purpose: Complete notification system integration
Implementation: Connect to unified notification service (from Story 10.2)

**Search Implementation with Filters in Cataloging Service**
Location: Cataloging service (Story 10.1 dependency)
Purpose: Advanced search and filtering capabilities
Implementation: Database query optimization with indexed search

**FileLifecycleService for Cleanup**
Location: Cataloging service file management
Purpose: Automated file lifecycle and cleanup management
Implementation: Scheduled cleanup jobs with configurable retention policies

**Authentication in Parser Admin**
Location: Parser admin interface
Purpose: Secure admin access and user management
Implementation: Session-based authentication with role-based access control

**Operation History Storage**
Location: Cross-service operation tracking
Purpose: Audit trail and operation monitoring
Implementation: Centralized history service with structured logging

**Job Tracking Implementation with Proper UUIDs**
Location: Multiple services requiring job coordination
Purpose: Distributed job tracking and correlation
Implementation: Unified job tracking service with UUID-based correlation

**Audio Duration Detection from Actual Files (2 occurrences)**
Location: File analysis and metadata extraction
Purpose: Real audio file analysis instead of estimated durations
Implementation: Audio library integration for precise duration calculation

**Version Mismatch Fix in sync_event_consumer**
Location: Event synchronization service
Purpose: Resolve dependency version conflicts
Implementation: Dependency analysis and compatibility updates

**Validation and Conversion Logic in CUE Generation Handler**
Location: CUE file generation and processing
Purpose: Complete CUE file validation and format conversion
Implementation: CUE parser integration with validation and error handling

### File Rename Proposal Integration
Connect metadata-driven rename suggestions:

```python
class FileRenameProposal:
    def __init__(self, metadata_repository: MetadataRepository):
        self.metadata_repo = metadata_repository

    async def generate_proposal(self, recording_id: UUID) -> RenameProposal:
        metadata = await self.metadata_repo.get_by_recording_id(recording_id)

        # Use metadata to suggest intelligent file name
        artist = self.extract_metadata_value(metadata, "artist")
        title = self.extract_metadata_value(metadata, "title")
        date = self.extract_metadata_value(metadata, "date")
        bpm = self.extract_metadata_value(metadata, "bpm")

        proposed_name = self.generate_name_from_metadata(
            artist, title, date, bpm
        )

        return RenameProposal(
            original_name=recording.file_name,
            proposed_name=proposed_name,
            confidence=self.calculate_confidence(metadata),
            metadata_used=metadata
        )
```

### Job Tracking System Implementation
Unified job tracking across all services:

```python
class JobTracker:
    def __init__(self, db_session: AsyncSession):
        self.db = db_session

    async def create_job(self, job_type: str, context: dict) -> UUID:
        job_id = uuid4()
        job = Job(
            id=job_id,
            job_type=job_type,
            status=JobStatus.CREATED,
            context=context,
            created_at=datetime.now(UTC)
        )
        self.db.add(job)
        await self.db.commit()
        return job_id

    async def update_status(self, job_id: UUID, status: JobStatus, result: dict = None):
        job = await self.db.get(Job, job_id)
        job.status = status
        job.result = result
        job.updated_at = datetime.now(UTC)
        await self.db.commit()

class Job:
    id: UUID
    job_type: str  # analysis, tracklist_generation, etc.
    status: JobStatus  # created, running, completed, failed
    context: dict  # Job-specific data
    result: dict  # Job result data
    created_at: datetime
    updated_at: datetime
    correlation_id: UUID  # For linking related jobs
```

### Audio Duration Detection Implementation
Real audio file analysis:

```python
import mutagen
from mutagen.mp3 import MP3
from mutagen.flac import FLAC
from mutagen.mp4 import MP4

class AudioDurationDetector:
    async def detect_duration(self, file_path: str) -> float:
        """
        Replace TODO: Audio duration detection from actual files
        Returns duration in seconds with high precision
        """
        try:
            # Use mutagen for accurate audio file analysis
            audio_file = mutagen.File(file_path)
            if audio_file is None:
                raise AudioFormatError(f"Unsupported audio format: {file_path}")

            duration = audio_file.info.length
            if duration is None:
                raise AudioAnalysisError(f"Could not determine duration: {file_path}")

            return float(duration)

        except Exception as e:
            logger.error(f"Audio duration detection failed: {file_path} - {e}")
            raise AudioDurationError(f"Duration detection failed: {e}")

    async def get_audio_metadata(self, file_path: str) -> dict:
        """Enhanced audio metadata extraction"""
        audio_file = mutagen.File(file_path)
        return {
            "duration": audio_file.info.length,
            "bitrate": getattr(audio_file.info, "bitrate", None),
            "sample_rate": getattr(audio_file.info, "sample_rate", None),
            "format": audio_file.mime[0] if audio_file.mime else "unknown"
        }
```

### Authentication System for Parser Admin
Secure admin interface:

```python
class AdminAuthenticationService:
    def __init__(self, session_manager: SessionManager):
        self.sessions = session_manager

    async def authenticate_user(self, username: str, password: str) -> Optional[User]:
        user = await self.get_user_by_username(username)
        if user and self.verify_password(password, user.password_hash):
            return user
        return None

    async def create_session(self, user: User) -> str:
        session_token = self.generate_secure_token()
        await self.sessions.store_session(session_token, user.id)
        return session_token

    async def validate_session(self, session_token: str) -> Optional[User]:
        user_id = await self.sessions.get_user_id(session_token)
        if user_id:
            return await self.get_user_by_id(user_id)
        return None

# FastAPI integration
@app.middleware("http")
async def admin_auth_middleware(request: Request, call_next):
    if request.url.path.startswith("/admin/"):
        session_token = request.cookies.get("session_token")
        if not session_token:
            return RedirectResponse("/admin/login")

        user = await auth_service.validate_session(session_token)
        if not user:
            return RedirectResponse("/admin/login")

        request.state.user = user

    return await call_next(request)
```

### Operation History Storage
Comprehensive operation auditing:

```python
class OperationHistoryService:
    def __init__(self, db_session: AsyncSession):
        self.db = db_session

    async def log_operation(
        self,
        operation_type: str,
        entity_type: str,
        entity_id: str,
        user_id: Optional[str] = None,
        details: dict = None
    ):
        history_entry = OperationHistory(
            id=uuid4(),
            operation_type=operation_type,  # create, update, delete, process
            entity_type=entity_type,  # recording, tracklist, metadata
            entity_id=entity_id,
            user_id=user_id,
            details=details or {},
            timestamp=datetime.now(UTC)
        )

        self.db.add(history_entry)
        await self.db.commit()

    async def get_history(
        self,
        entity_type: str = None,
        entity_id: str = None,
        operation_type: str = None,
        limit: int = 100
    ) -> List[OperationHistory]:
        query = select(OperationHistory)

        if entity_type:
            query = query.where(OperationHistory.entity_type == entity_type)
        if entity_id:
            query = query.where(OperationHistory.entity_id == entity_id)
        if operation_type:
            query = query.where(OperationHistory.operation_type == operation_type)

        query = query.order_by(OperationHistory.timestamp.desc()).limit(limit)

        result = await self.db.execute(query)
        return result.scalars().all()
```

### Version Mismatch Resolution
Dependency compatibility fixes:

```python
# Version compatibility checker
class VersionCompatibilityChecker:
    def __init__(self, compatibility_matrix: dict):
        self.compatibility = compatibility_matrix

    def check_compatibility(self, service_versions: dict) -> CompatibilityReport:
        conflicts = []
        warnings = []

        for service, version in service_versions.items():
            compatible_versions = self.compatibility.get(service, {})
            for dependent_service, dependent_version in service_versions.items():
                if dependent_service != service:
                    if not self.is_compatible(service, version, dependent_service, dependent_version):
                        conflicts.append(VersionConflict(
                            service_a=service,
                            version_a=version,
                            service_b=dependent_service,
                            version_b=dependent_version
                        ))

        return CompatibilityReport(conflicts=conflicts, warnings=warnings)

# sync_event_consumer version fixes
COMPATIBLE_VERSIONS = {
    "pydantic": ">=2.0,<3.0",
    "sqlalchemy": ">=2.0,<3.0",
    "redis": ">=4.0,<6.0",
    "rabbitmq": ">=4.0,<5.0"
}
```

### CUE Generation Validation and Conversion
Complete CUE file processing:

```python
class CUEGenerationHandler:
    def __init__(self, cue_parser: CUEParser):
        self.parser = cue_parser

    async def generate_and_validate_cue(
        self,
        tracklist: Tracklist,
        audio_file_path: str
    ) -> CUEFile:
        # Generate CUE content
        cue_content = self.generate_cue_content(tracklist, audio_file_path)

        # Validate CUE format
        validation_result = await self.validate_cue_format(cue_content)
        if not validation_result.is_valid:
            raise CUEValidationError(validation_result.errors)

        # Parse and convert if needed
        parsed_cue = self.parser.parse(cue_content)

        # Cache the generated CUE
        cue_file = CUEFile(
            content=cue_content,
            tracklist_id=tracklist.id,
            file_path=self.get_cue_file_path(tracklist.id),
            parsed_tracks=parsed_cue.tracks,
            validation_status="valid"
        )

        await self.cache_cue_file(cue_file)
        return cue_file

    def generate_cue_content(self, tracklist: Tracklist, audio_file: str) -> str:
        """Generate properly formatted CUE content"""
        cue_lines = [
            f'FILE "{audio_file}" WAVE',
            ""
        ]

        for i, track in enumerate(tracklist.tracks, 1):
            cue_lines.extend([
                f"  TRACK {i:02d} AUDIO",
                f'    TITLE "{track["title"]}"',
                f'    PERFORMER "{track["artist"]}"',
                f"    INDEX 01 {self.format_time(track['start_time'])}"
            ])

        return "\n".join(cue_lines)
```

### Technical Constraints
[Source: architecture/coding-standards.md, architecture/tech-stack.md]

- Use `uv run` for all Python commands
- SQLAlchemy ORM for all database operations
- RabbitMQ for inter-service communication
- Environment variables for configuration
- Structured logging for all operations
- All pre-commit hooks must pass
- Maintain backward compatibility during implementation

### Integration Points
Connect with existing and new services:
- File rename proposal → Metadata repository (Story 10.1)
- Job tracking → All processing services
- Audio duration → Analysis service (Story 10.3)
- Authentication → Parser admin interface
- Operation history → All services with audit requirements
- Notification integration → Unified notification service (Story 10.2)

### Performance Requirements
TODO implementation performance targets:
- Metadata integration: <200ms for rename proposals
- Job tracking operations: <100ms for status updates
- Audio duration detection: <2 seconds for typical files
- Authentication: <50ms for session validation
- Operation history logging: <100ms per entry
- CUE generation: <500ms for typical tracklists

## Testing
[Source: architecture/test-strategy-and-standards.md]

### Testing Requirements
- Test location: `tests/unit/` and `tests/integration/` across affected services
- Use pytest as testing framework
- Execute with `uv run pytest tests/ -v`
- Minimum 80% code coverage for all new implementations

### Testing Categories
1. **Metadata Integration Tests**: File rename proposals, metadata correlation
2. **Job Tracking Tests**: UUID generation, status updates, correlation
3. **Audio Analysis Tests**: Duration detection, format support, error handling
4. **Authentication Tests**: Login/logout, session management, authorization
5. **History Tests**: Operation logging, querying, audit trails
6. **Version Compatibility Tests**: Dependency validation, conflict resolution
7. **CUE Generation Tests**: Format validation, parsing, caching
8. **Integration Tests**: Cross-service TODO implementations
9. **Performance Tests**: Response times under load for all implementations

### Test Data Requirements
- Real audio files for duration detection testing
- Comprehensive metadata sets for rename proposal testing
- Authentication test users with various permission levels
- Version compatibility test matrices
- CUE file format validation test cases
- Operation history test scenarios across services

### Regression Testing
- Ensure existing functionality remains intact after TODO implementations
- Test backward compatibility with current service interfaces
- Validate that TODO implementations don't break existing workflows
- Performance regression testing for all affected endpoints

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-01 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-09-01 | 1.1 | Renumbered and updated cross-references in Epic reorganization | Development Team |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
Claude Opus 4.1 (claude-opus-4-1-20250805)

### Debug Log References
- Task 1: Metadata integration implementation
  - Found 5 TODO occurrences in file rename proposal components
  - Integrated MetadataRepository into message_interface.py and batch_processor.py
  - Added _determine_metadata_source method to proposal_generator.py
- Task 2: Job tracking system implementation
  - Created Job model with UUID generation and hierarchical support
  - Created JobRepository with full CRUD operations
  - Integrated job tracking into CueGenerationService
  - Added job status API endpoint
  - Created database migration for jobs table

### Completion Notes List
- Successfully integrated metadata repository with file rename proposal system
- Added metadata fetching from MetadataRepository in message interface and batch processor
- Implemented intelligent metadata source determination based on available fields
- Created comprehensive unit tests for metadata integration
- Implemented complete job tracking system with UUID generation
- Added job correlation support for tracking related jobs across services
- Integrated job tracking into CUE generation workflow
- Created comprehensive tests for job tracking system
- All tests passing

### File List
- services/analysis_service/src/file_rename_proposal/message_interface.py (Modified)
- services/analysis_service/src/file_rename_proposal/batch_processor.py (Modified)
- services/analysis_service/src/file_rename_proposal/proposal_generator.py (Modified)
- tests/unit/analysis_service/test_file_rename_proposal/test_metadata_integration.py (Created)
- shared/core_types/src/models.py (Modified - added Job model)
- shared/core_types/src/repositories.py (Modified - added JobRepository)
- alembic/versions/006_add_job_tracking.py (Created)
- services/tracklist_service/src/services/cue_generation_service.py (Modified)
- services/tracklist_service/src/api/cue_generation_api.py (Modified)
- tests/unit/shared/test_job_tracking.py (Created)

## QA Results
*Results from QA Agent review will be populated here after implementation*
