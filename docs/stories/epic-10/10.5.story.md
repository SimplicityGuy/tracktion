# Story 10.5: Replace Mock/Placeholder Implementations

## Status
Draft

## Story
**As a** system in production
**I want** all mock data replaced with real implementations
**So that** the system works with actual data

## Acceptance Criteria
1. All mock data returns replaced with database queries
2. Placeholder error handling replaced with proper recovery logic
3. Dummy data generation replaced with real data sources
4. Stub scrapers updated with actual selectors
5. All placeholder comments removed or implemented

## Tasks / Subtasks
- [ ] Replace mock data returns with database queries (AC: 1)
  - [ ] Identify all mock/fake data returns across all services
  - [ ] Replace mock recording data with database queries
  - [ ] Replace mock metadata with real metadata extraction
  - [ ] Replace mock tracklist data with real tracklist processing
  - [ ] Replace mock analysis results with real computation results
- [ ] Implement proper error handling and recovery (AC: 2)
  - [ ] Replace placeholder error handlers with comprehensive error recovery
  - [ ] Add proper exception handling with fallback strategies
  - [ ] Implement circuit breakers for external service failures
  - [ ] Add proper error logging and monitoring
- [ ] Replace dummy data generation with real sources (AC: 3)
  - [ ] Replace dummy BPM detection with real audio analysis
  - [ ] Replace fake genre classification with ML model results
  - [ ] Replace placeholder mood detection with actual analysis
  - [ ] Replace mock tracklist parsing with real parser integration
- [ ] Update scraper implementations with real selectors (AC: 4)
  - [ ] Update 1001tracklists scraper with current DOM selectors
  - [ ] Replace placeholder web scraping logic with working implementations
  - [ ] Add proper error handling for scraping failures
  - [ ] Implement rate limiting and respectful crawling practices
- [ ] Remove or implement all placeholder comments (AC: 5)
  - [ ] Find and catalog all placeholder/stub comments
  - [ ] Implement functionality for important placeholders
  - [ ] Remove placeholder comments that are no longer needed
  - [ ] Update documentation to reflect real implementations
- [ ] Add real-time data processing (AC: 1-5)
  - [ ] Replace batch placeholder processing with real-time workflows
  - [ ] Implement streaming data processing where appropriate
  - [ ] Add real-time status updates and progress tracking
  - [ ] Connect all services to real data flows
- [ ] Implement production-ready caching (AC: 1-3)
  - [ ] Replace in-memory placeholder caches with Redis
  - [ ] Add cache invalidation strategies
  - [ ] Implement cache warming for frequently accessed data
  - [ ] Add cache performance monitoring
- [ ] Write comprehensive tests for real implementations (AC: 1-5)
  - [ ] Test database queries with realistic data volumes
  - [ ] Test error handling and recovery mechanisms
  - [ ] Test real data processing pipelines
  - [ ] Test scraper implementations with real websites
  - [ ] Test performance with production-scale data

## Dev Notes

### Previous Story Insights
Story 10.5 is the final cleanup story for Epic 10, ensuring all mock, placeholder, and stub implementations are replaced with production-ready code. This story depends on all previous stories (10.1-10.4) as it ties together the real implementations across all services and removes development scaffolding.

### Mock/Placeholder Implementation Analysis
[Source: Epic 10 detailed analysis - Placeholder/Mock/Stub Implementations]

**Categories of Mock/Placeholder Code:**
- **Mock Data Returns**: Endpoints returning hard-coded test data instead of database queries
- **Placeholder Logic**: Error handlers and processors with "TODO: implement proper logic"
- **Dummy Data Generation**: Fake data generators for development/testing
- **Stub Implementations**: Incomplete scrapers and parsers with placeholder selectors
- **Development Scaffolding**: Temporary code that bypasses real functionality

### Mock Data Replacement Strategy
Replace all hard-coded mock returns with real database operations:

```python
# BEFORE: Mock data return
@app.get("/recordings/{recording_id}/metadata")
async def get_metadata(recording_id: UUID):
    # Mock data for development
    return {
        "bpm": 128,
        "key": "Am",
        "genre": "Electronic",
        "mood": "Energetic"
    }

# AFTER: Real database implementation
@app.get("/recordings/{recording_id}/metadata")
async def get_metadata(recording_id: UUID):
    metadata = await metadata_repository.get_by_recording_id(recording_id)
    if not metadata:
        # Trigger analysis if metadata doesn't exist
        await analysis_service.analyze_recording(recording_id)
        metadata = await metadata_repository.get_by_recording_id(recording_id)

    return {
        item.key: item.value for item in metadata
    }
```

### Error Handling Replacement
Transform placeholder error handling into comprehensive recovery systems:

```python
# BEFORE: Placeholder error handling
def process_audio_file(file_path: str):
    try:
        return analyze_audio(file_path)
    except Exception:
        # TODO: Implement proper error handling
        return None

# AFTER: Comprehensive error handling
async def process_audio_file(file_path: str) -> AudioAnalysisResult:
    try:
        return await analyze_audio(file_path)
    except FileNotFoundError:
        logger.error(f"Audio file not found: {file_path}")
        raise AudioFileError(f"File not found: {file_path}")
    except PermissionError:
        logger.error(f"Permission denied accessing: {file_path}")
        await file_permission_service.request_access(file_path)
        raise AudioAccessError(f"Permission denied: {file_path}")
    except AudioFormatError as e:
        logger.warning(f"Unsupported format {file_path}: {e}")
        return AudioAnalysisResult.unsupported_format(file_path)
    except Exception as e:
        logger.error(f"Unexpected error processing {file_path}: {e}")
        await error_notification_service.notify_error(e, file_path)
        raise AudioProcessingError(f"Processing failed: {e}")
```

### Dummy Data Generation Replacement
Replace fake data generators with real analysis results:

```python
# BEFORE: Dummy BPM detection
def detect_bpm(audio_file: str) -> float:
    # Dummy implementation - returns random BPM
    import random
    return random.randint(120, 140)

# AFTER: Real BPM detection
async def detect_bpm(audio_file: str) -> float:
    """Real BPM detection using librosa"""
    try:
        import librosa
        import numpy as np

        # Load audio file
        y, sr = librosa.load(audio_file)

        # Extract tempo
        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)

        # Validate tempo range
        if tempo < 60 or tempo > 200:
            logger.warning(f"Unusual BPM detected: {tempo} for {audio_file}")

        return float(tempo)

    except Exception as e:
        logger.error(f"BPM detection failed for {audio_file}: {e}")
        raise BPMDetectionError(f"Cannot detect BPM: {e}")
```

### Scraper Implementation Updates
Update web scrapers with current DOM selectors and robust error handling:

```python
# BEFORE: Stub scraper implementation
class TracklistScraper:
    def scrape_tracklist(self, url: str) -> List[Track]:
        # TODO: Implement actual scraping logic
        return [Track(title="Mock Track", artist="Mock Artist")]

# AFTER: Real scraper implementation
class TracklistScraper:
    def __init__(self, session_manager: HTTPSessionManager):
        self.session = session_manager
        self.rate_limiter = RateLimiter(requests_per_minute=60)

    async def scrape_tracklist(self, url: str) -> List[Track]:
        await self.rate_limiter.wait_for_slot()

        try:
            response = await self.session.get(url, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            tracks = []

            # Current DOM selectors for 1001tracklists.com
            track_elements = soup.select('.tlpTog')

            for element in track_elements:
                try:
                    title_elem = element.select_one('.trackValue[title]')
                    artist_elem = element.select_one('.trackArtist')
                    time_elem = element.select_one('.cueValueField')

                    if title_elem and artist_elem:
                        tracks.append(Track(
                            title=title_elem.get('title', '').strip(),
                            artist=artist_elem.text.strip(),
                            start_time=self.parse_time(time_elem.text if time_elem else '0:00')
                        ))

                except Exception as e:
                    logger.warning(f"Failed to parse track element: {e}")
                    continue

            if not tracks:
                raise ScrapingError(f"No tracks found at {url}")

            return tracks

        except requests.RequestException as e:
            logger.error(f"HTTP error scraping {url}: {e}")
            raise ScrapingError(f"Failed to fetch {url}: {e}")
        except Exception as e:
            logger.error(f"Parsing error for {url}: {e}")
            raise ScrapingError(f"Failed to parse {url}: {e}")
```

### Placeholder Comment Cleanup
Systematic cleanup of development comments:

```python
# Development script to find placeholder comments
PLACEHOLDER_PATTERNS = [
    "TODO:",
    "FIXME:",
    "HACK:",
    "XXX:",
    "Mock implementation",
    "Dummy data",
    "Placeholder",
    "Stub implementation",
    "In real implementation",
    "For development only"
]

async def find_placeholder_comments(directory: str) -> List[PlaceholderComment]:
    """Find all placeholder comments in codebase"""
    placeholders = []

    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                placeholders.extend(
                    await scan_file_for_placeholders(file_path)
                )

    return placeholders

async def generate_cleanup_report(placeholders: List[PlaceholderComment]) -> CleanupReport:
    """Generate report categorizing placeholders by priority"""
    return CleanupReport(
        critical=filter_critical_placeholders(placeholders),
        important=filter_important_placeholders(placeholders),
        cleanup_only=filter_cleanup_placeholders(placeholders),
        total_count=len(placeholders)
    )
```

### Production-Ready Caching Implementation
Replace in-memory caches with Redis-backed production caching:

```python
# BEFORE: In-memory placeholder cache
class InMemoryCache:
    def __init__(self):
        self._cache = {}  # Simple dict cache

    def get(self, key: str):
        return self._cache.get(key)

    def set(self, key: str, value: any):
        self._cache[key] = value

# AFTER: Production Redis cache
class ProductionCacheService:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.serializer = JSONSerializer()

    async def get(self, key: str, type_hint: Type = None):
        try:
            value = await self.redis.get(key)
            if value is None:
                return None

            deserialized = self.serializer.deserialize(value)
            if type_hint:
                return self.validate_type(deserialized, type_hint)
            return deserialized

        except redis.RedisError as e:
            logger.error(f"Cache get failed for {key}: {e}")
            return None

    async def set(self, key: str, value: any, ttl: int = 3600):
        try:
            serialized = self.serializer.serialize(value)
            await self.redis.setex(key, ttl, serialized)
        except redis.RedisError as e:
            logger.error(f"Cache set failed for {key}: {e}")

    async def invalidate_pattern(self, pattern: str):
        """Invalidate all keys matching pattern"""
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)
```

### Real-Time Data Processing
Replace batch placeholder processing with real-time workflows:

```python
# BEFORE: Batch placeholder processing
def process_files_batch():
    # Process files in batches every hour
    files = get_pending_files()
    for file in files:
        process_file(file)  # Placeholder processing

# AFTER: Real-time streaming processing
class RealTimeProcessor:
    def __init__(self, event_stream: EventStream):
        self.stream = event_stream
        self.processors = {
            'file_added': self.process_new_file,
            'metadata_extracted': self.process_metadata,
            'analysis_complete': self.process_analysis_results
        }

    async def start_processing(self):
        async for event in self.stream:
            processor = self.processors.get(event.type)
            if processor:
                try:
                    await processor(event)
                except Exception as e:
                    logger.error(f"Event processing failed {event.type}: {e}")
                    await self.handle_processing_error(event, e)
            else:
                logger.warning(f"No processor for event type: {event.type}")

    async def process_new_file(self, event: FileAddedEvent):
        """Real-time file processing"""
        file_path = event.file_path
        recording = await self.create_recording(file_path)

        # Trigger analysis pipeline
        await self.analysis_service.analyze_async(recording.id)

        # Notify other services
        await self.event_publisher.publish(RecordingCreatedEvent(
            recording_id=recording.id,
            file_path=file_path
        ))
```

### Service Integration Points
Connect all real implementations across services:

```python
# Real service integration replacing placeholder connections
class ServiceIntegrator:
    def __init__(self, service_registry: ServiceRegistry):
        self.services = service_registry

    async def integrate_analysis_pipeline(self):
        """Connect all analysis services with real implementations"""

        # File watcher → Cataloging service (real database operations)
        await self.connect_services(
            'file_watcher', 'cataloging_service',
            message_type='file_discovered',
            handler='catalog_new_file'
        )

        # Cataloging → Analysis service (real processing)
        await self.connect_services(
            'cataloging_service', 'analysis_service',
            message_type='recording_cataloged',
            handler='analyze_recording'
        )

        # Analysis → Notification service (real notifications)
        await self.connect_services(
            'analysis_service', 'notification_service',
            message_type='analysis_complete',
            handler='send_completion_notification'
        )
```

### Technical Constraints
[Source: architecture/coding-standards.md, architecture/tech-stack.md]

- Use `uv run` for all Python commands
- Replace all mock/placeholder code with production implementations
- Use Redis for production caching (not in-memory caches)
- Implement proper error handling with recovery strategies
- Add comprehensive logging for all real implementations
- All pre-commit hooks must pass
- Performance testing required for all replacements

### Performance Impact Assessment
Evaluate performance impact of replacing mocks with real implementations:

```python
class PerformanceImpactAnalyzer:
    async def analyze_replacement_impact(self, component: str) -> PerformanceReport:
        """Analyze performance impact of replacing mock with real implementation"""

        # Benchmark current mock performance
        mock_performance = await self.benchmark_mock_implementation(component)

        # Benchmark new real implementation
        real_performance = await self.benchmark_real_implementation(component)

        return PerformanceReport(
            component=component,
            mock_avg_time=mock_performance.avg_response_time,
            real_avg_time=real_performance.avg_response_time,
            performance_ratio=real_performance.avg_response_time / mock_performance.avg_response_time,
            memory_impact=real_performance.memory_usage - mock_performance.memory_usage,
            recommendation=self.generate_recommendation(mock_performance, real_performance)
        )
```

### Validation and Testing Strategy
Comprehensive validation of all real implementations:

```python
class MockReplacementValidator:
    async def validate_all_replacements(self) -> ValidationReport:
        """Validate that all mock replacements work correctly"""

        validation_results = []

        # Validate database integrations
        validation_results.extend(await self.validate_database_integrations())

        # Validate external service integrations
        validation_results.extend(await self.validate_external_integrations())

        # Validate error handling improvements
        validation_results.extend(await self.validate_error_handling())

        # Validate performance within acceptable bounds
        validation_results.extend(await self.validate_performance_impact())

        return ValidationReport(
            passed=len([r for r in validation_results if r.passed]),
            failed=len([r for r in validation_results if not r.passed]),
            results=validation_results
        )
```

### Production Readiness Checklist
Final checklist before deployment:

- [ ] All mock data returns replaced with database queries
- [ ] All placeholder error handlers replaced with comprehensive error handling
- [ ] All dummy data generation replaced with real analysis
- [ ] All stub implementations replaced with working code
- [ ] All development comments cleaned up or implemented
- [ ] Redis caching implemented for all cached operations
- [ ] Real-time processing implemented where appropriate
- [ ] Performance validated within acceptable bounds
- [ ] Error handling tested with realistic failure scenarios
- [ ] Integration testing completed across all services
- [ ] Load testing completed with production-scale data
- [ ] Monitoring and alerting configured for all new implementations

## Testing
[Source: architecture/test-strategy-and-standards.md]

### Testing Requirements
- Test location: `tests/unit/` and `tests/integration/` across all affected services
- Use pytest as testing framework
- Execute with `uv run pytest tests/ -v`
- Minimum 80% code coverage for all replacement implementations

### Testing Categories
1. **Mock Replacement Tests**: Validate real implementations work correctly
2. **Error Handling Tests**: Comprehensive error scenarios and recovery
3. **Performance Tests**: Ensure acceptable performance with real implementations
4. **Integration Tests**: End-to-end workflows with real data
5. **Scraper Tests**: Web scraping with real websites (or reasonable mocks)
6. **Cache Tests**: Redis caching behavior and invalidation
7. **Real-Time Processing Tests**: Streaming data processing workflows
8. **Load Tests**: Production-scale data volumes and concurrent operations

### Production Data Testing
- Use production-scale datasets for testing (anonymized/synthetic if needed)
- Test with realistic data volumes (10,000+ recordings, metadata, tracklists)
- Validate performance under realistic load conditions
- Test error handling with real failure scenarios
- Validate memory usage and resource consumption

### Regression Testing
- Ensure functionality remains intact after mock replacement
- Validate that API contracts remain unchanged
- Test backward compatibility with existing integrations
- Performance regression testing to ensure acceptable performance
- End-to-end regression testing across all user workflows

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-01 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-09-01 | 1.1 | Renumbered from 10.6 to 10.5 and updated references | Development Team |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
*To be filled by dev agent*

### Debug Log References
*To be filled by dev agent*

### Completion Notes List
*To be filled by dev agent*

### File List
*To be filled by dev agent*

## QA Results
*Results from QA Agent review will be populated here after implementation*
