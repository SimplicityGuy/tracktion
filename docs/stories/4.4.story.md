# Story 4.4: Batch Processing Support

## Story Details
- **Epic**: 4 - Build 1001tracklists.com API
- **Story Number**: 4.4
- **Status**: Approved
- **Created**: 2025-08-21
- **Updated**: 2025-08-21

## User Story
**As a** user with many mixes to catalog
**I want** to process multiple tracklists efficiently
**So that** I can build my catalog quickly

## Acceptance Criteria
- [ ] Queue multiple scraping requests with single API call
- [ ] Parallel processing with configurable rate limiting
- [ ] Real-time progress tracking for batch jobs
- [ ] Automatic error recovery for failed items
- [ ] Result aggregation with multiple export formats
- [ ] Support for 100+ URLs in single batch
- [ ] Processing rate of at least 10 tracklists per minute

## Technical Requirements

### Queue Management
- RabbitMQ integration for job queuing
- Priority levels for job processing
- Job deduplication to prevent redundant work
- Scheduled batch execution support

### Parallel Processing
- Configurable worker pool (1-10 workers)
- Per-domain concurrency limits
- Dynamic scaling based on system load
- Request interleaving for efficiency

### Progress Tracking
- Real-time updates via WebSocket
- Persistent progress storage
- Detailed status per item
- ETA calculation for batch completion

### Error Handling
- Automatic retry with exponential backoff
- Dead letter queue for persistent failures
- Different retry strategies per error type
- Manual retry interface

## Tasks

### Task 1: Design & Implement Job Queue System
**Description**: Extend RabbitMQ configuration for batch job management
**Acceptance Criteria**:
- [ ] Job priority levels (immediate, normal, low)
- [ ] Job deduplication logic preventing duplicates
- [ ] Scheduled job execution with cron syntax
- [ ] Job persistence across service restarts

**Implementation**:
```python
# services/tracklist_service/src/queue/batch_queue.py
class BatchJobQueue:
    def enqueue_batch(self, urls: List[str], priority: str, user_id: str) -> str
    def deduplicate_jobs(self, jobs: List[Job]) -> List[Job]
    def schedule_batch(self, urls: List[str], cron_expression: str) -> str
```

### Task 2: Build Batch Request API Endpoints
**Description**: Create RESTful endpoints for batch operations
**Acceptance Criteria**:
- [ ] POST endpoint accepting multiple URLs
- [ ] Batch status tracking endpoint
- [ ] Cancellation and pause/resume support
- [ ] Batch templates for common operations

**Implementation**:
```python
# services/tracklist_service/src/api/batch_endpoints.py
@router.post("/api/v1/batch")
async def create_batch(request: BatchRequest) -> BatchResponse:
    """Submit multiple URLs for processing"""

@router.get("/api/v1/batch/{batch_id}/status")
async def get_batch_status(batch_id: str) -> BatchStatus:
    """Get detailed status of batch job"""

@router.post("/api/v1/batch/{batch_id}/cancel")
async def cancel_batch(batch_id: str) -> dict:
    """Cancel a running batch job"""
```

### Task 3: Implement Parallel Processing Engine
**Description**: Create efficient parallel processing system
**Acceptance Criteria**:
- [ ] Worker pool management with auto-scaling
- [ ] Domain-specific concurrency limits
- [ ] Load-based worker allocation
- [ ] Request interleaving optimization

**Implementation**:
```python
# services/tracklist_service/src/workers/parallel_processor.py
class ParallelProcessor:
    def __init__(self, min_workers: int = 1, max_workers: int = 10):
        self.worker_pool = WorkerPool(min_workers, max_workers)

    async def process_batch(self, jobs: List[Job]) -> BatchResult:
        """Process multiple jobs in parallel with rate limiting"""

    def adjust_worker_count(self, load_metrics: dict) -> None:
        """Dynamically scale workers based on load"""
```

### Task 4: Add Intelligent Rate Limiting
**Description**: Implement adaptive rate limiting for batch operations
**Acceptance Criteria**:
- [ ] Adaptive rate adjustment based on response times
- [ ] Per-domain rate configurations
- [ ] Backpressure mechanisms preventing overload
- [ ] Request scheduling optimizer

**Implementation**:
```python
# services/tracklist_service/src/rate_limiting/batch_limiter.py
class BatchRateLimiter:
    def calculate_optimal_rate(self, domain: str, metrics: dict) -> float
    def apply_backpressure(self, queue_depth: int) -> None
    def schedule_requests(self, requests: List[Request]) -> List[ScheduledRequest]
```

### Task 5: Create Progress Tracking System
**Description**: Implement real-time progress monitoring
**Acceptance Criteria**:
- [ ] WebSocket endpoint for live updates
- [ ] Detailed metrics per item
- [ ] Progress persistence in Redis
- [ ] Email/webhook notifications on completion

**Implementation**:
```python
# services/tracklist_service/src/progress/tracker.py
class ProgressTracker:
    async def update_progress(self, batch_id: str, item_id: str, status: str) -> None
    async def calculate_eta(self, batch_id: str) -> datetime
    async def broadcast_update(self, batch_id: str, update: dict) -> None

# WebSocket endpoint
@router.websocket("/ws/batch/{batch_id}/progress")
async def progress_websocket(websocket: WebSocket, batch_id: str):
    """Stream real-time progress updates"""
```

### Task 6: Build Error Recovery & Retry Logic
**Description**: Implement robust error handling for batch operations
**Acceptance Criteria**:
- [ ] Exponential backoff with jitter
- [ ] Dead letter queue for failed items
- [ ] Strategy-based retry logic
- [ ] Manual retry interface

**Implementation**:
```python
# services/tracklist_service/src/retry/batch_retry.py
class BatchRetryManager:
    def get_retry_strategy(self, error_type: str) -> RetryStrategy
    async def retry_failed_items(self, batch_id: str, item_ids: List[str]) -> dict
    def move_to_dlq(self, job: Job) -> None
```

### Task 7: Implement Result Aggregation
**Description**: Create comprehensive result management system
**Acceptance Criteria**:
- [ ] Efficient result storage for large batches
- [ ] Streaming API for large result sets
- [ ] Multiple export formats (JSON, CSV, CUE)
- [ ] Result validation and quality scoring

**Implementation**:
```python
# services/tracklist_service/src/results/aggregator.py
class ResultAggregator:
    async def aggregate_batch_results(self, batch_id: str) -> BatchResults
    async def stream_results(self, batch_id: str) -> AsyncIterator[dict]
    def export_results(self, results: BatchResults, format: str) -> bytes
    def calculate_batch_quality_score(self, results: BatchResults) -> float
```

### Task 8: Testing & Performance Optimization
**Description**: Comprehensive testing and optimization
**Acceptance Criteria**:
- [ ] Load tests with 1000+ URL batches
- [ ] Stress tests with failure scenarios
- [ ] Performance benchmarks documented
- [ ] API documentation with examples

**Test Coverage**:
```python
# tests/integration/tracklist_service/test_batch_processing.py
async def test_large_batch_processing():
    """Test processing 100+ URLs efficiently"""

async def test_parallel_processing_limits():
    """Verify rate limiting and concurrency controls"""

async def test_batch_error_recovery():
    """Test automatic retry and error handling"""

async def test_progress_tracking_accuracy():
    """Verify real-time progress updates"""
```

## Definition of Done
- [ ] All tasks completed and tested
- [ ] Unit test coverage > 85% for new code
- [ ] Integration tests for all batch scenarios
- [ ] Load testing completed (1000+ URLs)
- [ ] WebSocket progress tracking verified
- [ ] Documentation with API examples
- [ ] Code reviewed and approved
- [ ] Successfully processes 100 URLs in under 10 minutes

## Performance Targets
- Batch submission: < 500ms for 100 URLs
- Processing rate: â‰¥ 10 tracklists/minute
- Progress update latency: < 100ms
- Result aggregation: < 2s for 100 items
- Memory usage: < 500MB for 1000 URL batch

## Risk Mitigation
- **Risk**: Memory exhaustion with large batches
  - **Mitigation**: Streaming processing, result pagination
- **Risk**: Uneven load distribution
  - **Mitigation**: Intelligent work stealing, dynamic scaling
- **Risk**: Progress tracking overhead
  - **Mitigation**: Batched updates, efficient storage

## Notes
- Consider implementing batch templates for common DJ sets or events
- Future enhancement: ML-based priority prediction
- Consider adding batch scheduling UI in future iteration
