# Story 2.2: Research Spike - Audio Analysis Libraries and Techniques

## Story Information
- **Epic**: 2 - Metadata Analysis & Naming
- **Story Number**: 2.2
- **Status**: Draft
- **Created**: 2025-08-17
- **Updated**: 2025-08-17

## Story Statement
**As a** technical lead,
**I want** to research and validate the best approaches for BPM detection and mood/genre analysis,
**so that** we can implement Stories 2.3 and 2.4 with confidence in our technical choices.

## Acceptance Criteria
1. Evaluate Essentia, librosa, madmom, and other relevant libraries for:
   - BPM detection accuracy and performance
   - Mood/genre classification capabilities
   - Key detection algorithms
   - Memory and CPU/GPU requirements
2. Prototype BPM detection with temporal analysis:
   - Implement BPM detection per time window (e.g., every 10 seconds)
   - Calculate start/end BPM for tracks with tempo changes
   - Determine average BPM with confidence scores
3. Validate Essentia's pre-trained models:
   - Test mood classification models on sample dataset
   - Evaluate genre detection accuracy
   - Assess model loading time and memory footprint
4. Research musical key detection approaches:
   - Compare available algorithms and libraries
   - Test accuracy on various musical genres
5. Document findings with:
   - Recommended library choices with justification
   - Performance benchmarks
   - Implementation guidelines
   - Resource requirements
   - Sample code for each approach
6. Create decision matrix comparing all options
7. Provide clear recommendation for production implementation

## Research Deliverables
- Technical report with benchmarks and recommendations
- Prototype code for BPM temporal analysis
- Validated approach for mood/genre detection
- Key detection algorithm selection
- Updated technical requirements for Stories 2.3 and 2.4

## Dev Notes

### Previous Story Insights
From Story 2.1 implementation:
- Analysis service successfully created at `services/analysis_service/`
- RabbitMQ consumer pattern established and working
- Basic metadata extraction using mutagen implemented
- Database integration with PostgreSQL and Neo4j functioning
- Error handling patterns and logging infrastructure in place
- Docker containerization working with Python 3.13
- All Python operations must use `uv` command (never use pip or python directly)
[Source: Previous story Dev Agent Record]

### Technical Stack
- **Language**: Python 3.13
- **Package Management**: uv (MANDATORY - never use pip or python directly)
- **Containerization**: Docker with Python 3.13 base image
- **Testing**: pytest (latest) with pytest-cov for coverage
- **Linting**: ruff (latest)
- **Type Checking**: mypy (latest)
[Source: architecture/tech-stack.md]

### Project Structure for Research
Research artifacts should be created at:
- `research/audio_analysis/` - Main research directory
- `research/audio_analysis/bpm_detection/` - BPM detection prototypes
- `research/audio_analysis/mood_genre/` - Mood and genre classification tests
- `research/audio_analysis/key_detection/` - Musical key detection experiments
- `research/audio_analysis/benchmarks/` - Performance benchmarking results
- `research/audio_analysis/reports/` - Technical reports and documentation
- `research/audio_analysis/sample_data/` - Sample audio files for testing
[Source: architecture/source-tree.md]

### Coding Standards for Research Code
- Maximum line length: 120 characters
- All Python execution via uv: `uv run python`, `uv run pytest`, etc.
- Type hints required for all function signatures
- Docstrings for all public methods
- Use environment variables for any API keys or credentials
- Document all findings with clear metrics and evidence
[Source: architecture/coding-standards.md]

### Testing Requirements for Prototypes
- Each prototype should include basic unit tests
- Test files named `test_*.py` in appropriate subdirectories
- Use `uv run pytest` for test execution
- Document test results with accuracy metrics
- Include sample audio files or references for reproducibility
[Source: architecture/test-strategy-and-standards.md]

### Libraries to Evaluate

#### Essentia
- **Purpose**: Comprehensive audio analysis library with pre-trained models
- **Capabilities**: BPM, key, mood, genre, danceability, voice/instrumental detection
- **Models**: MusiCNN, VGGish, Discogs EffNet for various classification tasks
- **Considerations**: Requires TensorFlow for model inference, large model files (~2-3GB)
[Source: Epic 2 requirements]

#### librosa
- **Purpose**: Python library for music and audio analysis
- **Capabilities**: BPM detection, onset detection, spectral analysis, key detection
- **Considerations**: Pure Python, good for prototyping, widely used in research
[No specific guidance found in architecture docs]

#### madmom
- **Purpose**: Audio signal processing library with focus on music information retrieval
- **Capabilities**: Beat tracking, tempo estimation, onset detection
- **Considerations**: Uses neural networks for some tasks, good accuracy for BPM
[No specific guidance found in architecture docs]

#### Additional Libraries to Consider
- aubio: Lightweight C library with Python bindings
- music21: Music theory and analysis (more focused on symbolic music)
- pyAudioAnalysis: Audio feature extraction and classification
[No specific guidance found in architecture docs]

### Performance Benchmarking Requirements
- Measure processing time for various file sizes (1MB, 10MB, 100MB, 500MB)
- Test with different audio formats (MP3, FLAC, WAV, M4A)
- Monitor memory usage during processing
- Test CPU vs GPU performance where applicable
- Document accuracy metrics with ground truth data
- Create reproducible benchmark suite
[Source: Story acceptance criteria]

### Model Management Considerations
- Research model download and caching strategies
- Evaluate model versioning approaches
- Consider containerization of models with services
- Document model file sizes and memory requirements
- Test model loading times and optimization techniques
[Source: Epic 2 Technical Considerations]

### Integration Considerations
- Research should consider how libraries will integrate with existing analysis_service
- Evaluate compatibility with Python 3.13 and uv package manager
- Consider Docker image size implications with large models
- Test concurrent processing capabilities
- Evaluate streaming vs batch processing approaches
[Source: Previous story implementation patterns]

## Tasks / Subtasks

### 1. Set Up Research Environment (Foundation)
- [ ] Create `research/audio_analysis/` directory structure
- [ ] Create `research/audio_analysis/pyproject.toml` with all libraries to test
- [ ] Install essentia, librosa, madmom, aubio via uv
- [ ] Set up Jupyter notebooks for interactive experimentation (optional)
- [ ] Create sample dataset with various genres and formats
- [ ] Document hardware specifications for benchmarking baseline

### 2. BPM Detection Evaluation (AC: 1, 2)
- [ ] Create `research/audio_analysis/bpm_detection/compare_bpm.py`
- [ ] Implement BPM detection using librosa.beat.beat_track
- [ ] Implement BPM detection using madmom.features.tempo
- [ ] Implement BPM detection using essentia.standard.RhythmExtractor2013
- [ ] Implement BPM detection using aubio.tempo
- [ ] Create temporal analysis prototype with windowed BPM calculation
- [ ] Test with electronic, classical, jazz, and variable tempo tracks
- [ ] Measure accuracy against known BPM tracks
- [ ] Document processing times and memory usage

### 3. Essentia Model Validation (AC: 3)
- [ ] Create `research/audio_analysis/mood_genre/essentia_models.py`
- [ ] Download and test MusiCNN models for mood detection
- [ ] Test VGGish models for audio classification
- [ ] Evaluate Discogs EffNet for genre detection
- [ ] Measure model loading times and memory footprint
- [ ] Test inference speed on various track lengths
- [ ] Document accuracy on diverse music genres
- [ ] Create model management strategy recommendations

### 4. Musical Key Detection Research (AC: 4)
- [ ] Create `research/audio_analysis/key_detection/compare_keys.py`
- [ ] Test Essentia's KeyExtractor and HPCP algorithms
- [ ] Test librosa's chroma features for key detection
- [ ] Evaluate madmom's key detection if available
- [ ] Compare accuracy across classical, pop, electronic genres
- [ ] Test handling of key changes within tracks
- [ ] Document confidence scores and error rates

### 5. Performance Benchmarking Suite (AC: 1, 5)
- [ ] Create `research/audio_analysis/benchmarks/benchmark_suite.py`
- [ ] Implement standardized test harness for all libraries
- [ ] Test with files of varying sizes (1MB to 500MB)
- [ ] Test different formats (MP3, FLAC, WAV, M4A)
- [ ] Monitor CPU usage, memory consumption, processing time
- [ ] Test parallel processing capabilities
- [ ] Create performance comparison charts
- [ ] Document resource requirements for production

### 6. Integration Feasibility Testing (AC: 5)
- [ ] Test library compatibility with Python 3.13
- [ ] Verify installation via uv package manager
- [ ] Test concurrent processing scenarios
- [ ] Evaluate Docker image size with each library
- [ ] Test streaming vs batch processing approaches
- [ ] Document integration challenges and solutions

### 7. Create Decision Matrix (AC: 6)
- [ ] Create comprehensive comparison spreadsheet
- [ ] Score each library on: accuracy, performance, features, ease of use
- [ ] Weight factors based on project priorities
- [ ] Include total cost of ownership (compute, storage, complexity)
- [ ] Document pros/cons for each option
- [ ] Create visual comparison charts

### 8. Write Technical Report (AC: 5, 7)
- [ ] Create `research/audio_analysis/reports/technical_report.md`
- [ ] Document all findings with evidence and metrics
- [ ] Include sample code for recommended approaches
- [ ] Provide clear recommendations for Stories 2.3 and 2.4
- [ ] Include implementation guidelines and best practices
- [ ] Add troubleshooting section for common issues
- [ ] Create executive summary with key decisions

### 9. Prototype Code Documentation (Research Deliverables)
- [ ] Clean and document all prototype code
- [ ] Create README for each prototype directory
- [ ] Include instructions for reproducing results
- [ ] Package sample data or provide download links
- [ ] Create requirements.txt or pyproject.toml for each prototype
- [ ] Add inline comments explaining algorithms and approaches

### 10. Validation and Peer Review
- [ ] Run all prototypes with fresh environment
- [ ] Verify all benchmarks are reproducible
- [ ] Cross-check accuracy metrics with external sources
- [ ] Document any assumptions or limitations
- [ ] Prepare presentation materials for team review
- [ ] Update Stories 2.3 and 2.4 requirements based on findings

## Implementation Guidance

### Critical Path Considerations
1. **Environment Setup First**: Ensure all libraries can be installed and work with Python 3.13
2. **Quick Wins**: Start with basic BPM detection to validate approach
3. **Model Download**: Essentia models are large - test download/caching strategy early
4. **Benchmark Consistency**: Use same audio files across all tests for fair comparison

### Common Pitfalls to Avoid
1. **Version Conflicts**: Some audio libraries may have conflicting dependencies
2. **Memory Issues**: Large models + large audio files can exhaust memory
3. **Accuracy Assumptions**: Don't assume one library is best for all genres
4. **GPU Dependencies**: Some libraries require CUDA - document requirements clearly
5. **Sample Bias**: Test with diverse music genres, not just electronic/pop

### Recommended Implementation Order
1. Set up research environment with all libraries
2. Create common test dataset with ground truth
3. Implement BPM detection comparison first (simpler, faster)
4. Then tackle mood/genre models (more complex, larger)
5. Key detection can be done in parallel
6. Run comprehensive benchmarks after individual tests
7. Write report with all evidence gathered

## Project Structure Notes
The research directory should be self-contained with its own dependencies and documentation. While it's separate from the main application code, findings will directly inform the implementation of Stories 2.3 and 2.4. Consider creating a separate Docker image for research to avoid polluting the main application dependencies.

### Research Output Structure
```
research/audio_analysis/
├── pyproject.toml              # All research dependencies
├── README.md                   # Research overview and instructions
├── bpm_detection/
│   ├── compare_bpm.py
│   ├── temporal_analysis.py
│   └── test_bpm_accuracy.py
├── mood_genre/
│   ├── essentia_models.py
│   ├── model_evaluation.py
│   └── test_classifications.py
├── key_detection/
│   ├── compare_keys.py
│   └── test_key_accuracy.py
├── benchmarks/
│   ├── benchmark_suite.py
│   ├── performance_results.json
│   └── charts/
├── reports/
│   ├── technical_report.md
│   ├── decision_matrix.xlsx
│   └── executive_summary.md
└── sample_data/
    ├── README.md              # How to obtain test files
    └── ground_truth.csv       # Known values for validation
```

## Success Metrics
- All major audio analysis libraries evaluated with quantitative metrics
- Clear recommendation provided for each analysis type (BPM, mood, genre, key)
- Performance benchmarks show feasibility for 1000+ files/hour processing
- Integration path with existing analysis_service is validated
- Technical report provides actionable implementation guidance
- Decision matrix justifies technology choices with data
- Prototype code demonstrates recommended approaches

## Dev Agent Record
_This section will be filled by the implementing developer/agent_

### Implementation Started
- Date:
- Agent/Developer:

### Completion Notes
- Date:
- Final Status:
- Key Findings:
- Deviations from Plan:

### Debug Log References
- Research experiments:
- Benchmark results:
- Technical decisions:
