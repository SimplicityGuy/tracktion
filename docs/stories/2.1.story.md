# Story 2.1: Analysis Service Setup & Basic Metadata Extraction

## Story Information
- **Epic**: 2 - Metadata Analysis & Naming
- **Story Number**: 2.1
- **Status**: Completed
- **Created**: 2025-08-16
- **Updated**: 2025-08-17
- **Completed**: 2025-08-17

## Story Statement
**As a** music-loving developer,
**I want** a functional analysis service that can extract basic metadata from audio files,
**so that** I can understand the technical properties and embedded information of my music collection.

## Acceptance Criteria
1. The analysis_service is created as a containerized Python service with proper project structure.
2. The service subscribes to RabbitMQ queues to receive file processing requests from the cataloging service.
3. Basic metadata extraction is implemented using mutagen or similar library (title, artist, album, duration, bitrate, sample rate, file format).
4. Extracted metadata is stored in both PostgreSQL (via Metadata model) and Neo4j (as graph nodes and relationships).
5. The service handles common audio formats (MP3, FLAC, WAV, M4A at minimum).
6. Error handling is implemented for corrupted or unsupported files.
7. Unit tests cover metadata extraction logic with sample audio files.
8. Integration tests verify end-to-end flow from message consumption to database storage.

## Dev Notes

### Previous Story Insights
From Story 1.2 implementation:
- Database models (Recording, Metadata, Tracklist) are implemented in `shared/core_types/src/models.py`
- Repository pattern established in `shared/core_types/src/repositories.py` and `shared/core_types/src/neo4j_repository.py`
- Database connection management with retry logic in `shared/core_types/src/database.py`
- SQLAlchemy and Alembic are configured and working
- Neo4j constraints and indexes are in place
- Test coverage achieved 89% with comprehensive unit and integration tests
- All Python operations must use `uv` command (never use pip or python directly)
[Source: Previous story Dev Agent Record]

### Service Architecture
Based on the microservices architecture, the analysis_service will:
- Be a separate containerized service in `services/analysis_service/`
- Communicate via RabbitMQ message queue for decoupled, asynchronous processing
- Follow the event-driven pattern where file events trigger analysis workflows
[Source: architecture/high-level-architecture.md]

### Project Structure
The analysis_service should be created at:
- `services/analysis_service/src/` - Source code location
- `services/analysis_service/Dockerfile` - Container definition
- `services/analysis_service/pyproject.toml` - Service dependencies
- `tests/unit/services/analysis/` - Unit test location
- `tests/integration/test_analysis_service.py` - Integration test location
[Source: architecture/source-tree.md]

### Data Models
The service will use existing models from `shared/core_types/src/models.py`:

**Metadata Model** (for storing extracted metadata):
- `id`: UUID (primary key, auto-generated)
- `recording_id`: UUID (foreign key to Recording)
- `key`: String (255 chars) - metadata name (e.g., "title", "artist", "album", "bitrate")
- `value`: Text - metadata value
- Indexed on: recording_id, key
[Source: architecture/data-models-refined-and-finalized.md]

**Neo4j Graph Structure**:
- Nodes: Recording, Metadata
- Relationships: HAS_METADATA (Recording -> Metadata)
- Properties on relationships can store additional context
[Source: architecture/database-schema-refined-and-finalized.md]

### Message Queue Configuration
RabbitMQ integration requirements:
- Service must subscribe to appropriate queue for file processing messages
- Use correlation IDs for end-to-end tracing across services
- Implement proper error handling and message acknowledgment
- No direct HTTP calls between services - all communication via RabbitMQ
[Source: architecture/high-level-architecture.md, architecture/coding-standards.md]

### Error Handling Requirements
- Implement centralized error-handling middleware or decorator
- Use structured logging (JSON format) with correlation IDs
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Every log must include: timestamp, service name, log level, correlation ID
- Implement retry mechanism with exponential backoff for transient errors
- Define custom exceptions hierarchy (e.g., InvalidAudioFileError, MetadataExtractionError)
[Source: architecture/error-handling-strategy.md]

### Technical Stack
- **Language**: Python 3.13
- **ORM**: SQLAlchemy (latest) - already configured in shared/core_types
- **Message Queue**: RabbitMQ 4.0
- **Databases**: PostgreSQL 17, Neo4j 5.26
- **Testing**: pytest (latest) with pytest-cov for coverage
- **Package Management**: uv (MANDATORY - never use pip or python directly)
- **Linting**: ruff (latest)
- **Type Checking**: mypy (latest)
[Source: architecture/tech-stack.md]

### Testing Requirements
- Unit tests required for all public methods using pytest
- Test files named `test_*.py` in `tests/unit/` directory
- Integration tests in `tests/integration/`
- Use `uv run pytest` for test execution
- Coverage goal: minimum 80%
- Test edge cases and error conditions
- Use pytest fixtures for database sessions
- Create separate test databases for isolation
- Mock external services for unit tests
[Source: architecture/test-strategy-and-standards.md]

### Coding Standards
- Maximum line length: 120 characters
- All Python execution via uv: `uv run python`, `uv run pytest`, etc.
- Type hints required for all function signatures
- Docstrings for all public methods
- Connection strings from environment variables only
- No direct database queries outside repository layer
- Inter-service communication via RabbitMQ only
[Source: architecture/coding-standards.md]

### Audio Processing Libraries
No specific audio processing libraries are mandated in the architecture documents. The story requirements mention "mutagen or similar library" for metadata extraction. Selection should be based on:
- Support for required formats (MP3, FLAC, WAV, M4A)
- Performance and memory efficiency
- Active maintenance and community support
- Compatibility with Python 3.13
[Source: No specific guidance found in architecture docs]

### Environment Configuration
Required environment variables:
- `DATABASE_URL` - PostgreSQL connection string
- `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD` - Neo4j connection
- `RABBITMQ_URL` - RabbitMQ connection string
- All must be added to `.env.example` with placeholder values
[Source: Previous story implementation patterns]

## Tasks / Subtasks

### 1. Create Analysis Service Structure (AC: 1)
- [ ] Create `services/analysis_service/` directory structure
- [ ] Create `services/analysis_service/src/__init__.py`
- [ ] Create `services/analysis_service/pyproject.toml` with dependencies
- [ ] Add mutagen (or chosen audio library) to dependencies
- [ ] Add pika for RabbitMQ communication
- [ ] Create `services/analysis_service/Dockerfile` based on Python 3.13

### 2. Implement RabbitMQ Consumer (AC: 2)
- [ ] Create `services/analysis_service/src/message_consumer.py`
- [ ] Implement connection to RabbitMQ using pika
- [ ] Create queue subscription logic with proper error handling
- [ ] Implement message acknowledgment pattern
- [ ] Add correlation ID tracking for tracing
- [ ] Create retry mechanism with exponential backoff

### 3. Implement Metadata Extraction (AC: 3, 5)
- [ ] Create `services/analysis_service/src/metadata_extractor.py`
- [ ] Implement audio file reading using mutagen or chosen library
- [ ] Extract basic metadata: title, artist, album, duration
- [ ] Extract technical metadata: bitrate, sample rate, file format
- [ ] Support MP3, FLAC, WAV, M4A formats
- [ ] Create data validation and sanitization logic

### 4. Integrate Database Storage (AC: 4)
- [ ] Create `services/analysis_service/src/storage_handler.py`
- [ ] Import and use existing repositories from shared/core_types
- [ ] Implement PostgreSQL storage via MetadataRepository
- [ ] Implement Neo4j storage via Neo4jRepository
- [ ] Create HAS_METADATA relationships in Neo4j
- [ ] Ensure transactional consistency between databases

### 5. Implement Error Handling (AC: 6)
- [ ] Create `services/analysis_service/src/exceptions.py` with custom exceptions
- [ ] Define InvalidAudioFileError, MetadataExtractionError, etc.
- [ ] Implement error handling decorator/middleware
- [ ] Add structured logging with JSON format
- [ ] Handle corrupted files gracefully
- [ ] Handle unsupported formats with clear error messages

### 6. Create Main Service Entry Point (AC: 1, 2)
- [ ] Create `services/analysis_service/src/main.py`
- [ ] Initialize database connections using shared/core_types
- [ ] Set up RabbitMQ consumer
- [ ] Implement graceful shutdown handling
- [ ] Add health check endpoint/mechanism
- [ ] Configure logging with appropriate levels

### 7. Write Unit Tests (AC: 7)
- [ ] Create `tests/unit/services/analysis/test_metadata_extractor.py`
- [ ] Test metadata extraction for each supported format
- [ ] Test error handling for corrupted files
- [ ] Test error handling for unsupported formats
- [ ] Create sample audio files or use mocks
- [ ] Achieve minimum 80% code coverage

### 8. Write Integration Tests (AC: 8)
- [ ] Create `tests/integration/test_analysis_service.py`
- [ ] Test complete flow from RabbitMQ message to database storage
- [ ] Test PostgreSQL metadata storage
- [ ] Test Neo4j relationship creation
- [ ] Test error recovery and retry mechanisms
- [ ] Test service startup and shutdown

### 9. Container Configuration (AC: 1)
- [ ] Configure Dockerfile with Python 3.13 base image
- [ ] Install system dependencies for audio processing
- [ ] Set up proper user permissions
- [ ] Configure environment variable handling
- [ ] Add service to docker-compose.yaml
- [ ] Test container build and execution

### 10. Documentation and Configuration
- [ ] Update `.env.example` with new environment variables
- [ ] Document service API/message format in README
- [ ] Add service description to main project README
- [ ] Document audio format limitations and requirements
- [ ] Create troubleshooting guide for common issues

## Implementation Guidance

### Critical Path Considerations
1. **Database Integration First**: Ensure you can connect to existing databases before implementing extraction
2. **Message Queue Setup**: Get RabbitMQ consumer working early to enable integration testing
3. **Library Selection**: Evaluate and choose audio library (mutagen vs alternatives) before deep implementation
4. **Error Handling Early**: Implement error patterns from the start to avoid retrofitting

### Common Pitfalls to Avoid
1. **Memory Management**: Large audio files can consume significant memory - implement streaming if needed
2. **Format Assumptions**: Don't assume all files have complete metadata - handle missing fields gracefully
3. **Queue Acknowledgment**: Only acknowledge messages after successful processing and storage
4. **Connection Pooling**: Reuse database connections from shared/core_types, don't create new ones
5. **Direct Python Execution**: Always use `uv run` for any Python commands

### Recommended Implementation Order
1. Set up service structure and dependencies
2. Implement basic RabbitMQ consumer
3. Create simple metadata extraction for one format (e.g., MP3)
4. Integrate with existing database repositories
5. Add error handling and logging
6. Expand to support all required formats
7. Write comprehensive tests
8. Containerize and integrate with docker-compose

## Project Structure Notes
The analysis_service will follow the established monorepo pattern within the services directory. It will import shared code from `shared/core_types` for database models and repositories, preventing code duplication. The service's Dockerfile will be self-contained while the docker-compose.yaml at the infrastructure level will orchestrate all services together.

### Package Dependencies
The `services/analysis_service/pyproject.toml` should include:
```toml
[project]
name = "analysis_service"
version = "0.1.0"
dependencies = [
    "mutagen>=1.47.0",  # or alternative audio library
    "pika>=1.3.0",  # RabbitMQ client
    "python-dotenv>=1.0.0",
    "pydantic>=2.0.0",  # for data validation
    "shared-core-types",  # internal package
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-cov>=4.0.0",
    "pytest-asyncio>=0.23.0",
    "mypy>=1.8.0",
    "ruff>=0.1.0",
]
```

## Code Examples and Patterns

### Message Consumer Pattern Example
```python
import pika
import json
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

class MessageConsumer:
    def __init__(self, rabbitmq_url: str, queue_name: str):
        self.connection = pika.BlockingConnection(
            pika.URLParameters(rabbitmq_url)
        )
        self.channel = self.connection.channel()
        self.queue_name = queue_name

    def process_message(self, ch, method, properties, body):
        try:
            message = json.loads(body)
            correlation_id = properties.correlation_id
            logger.info(f"Processing message {correlation_id}")

            # Process the message
            self.handle_file_analysis(message)

            # Acknowledge successful processing
            ch.basic_ack(delivery_tag=method.delivery_tag)
        except Exception as e:
            logger.error(f"Error processing message: {e}")
            # Reject and requeue for retry
            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
```

### Metadata Extraction Pattern Example
```python
from mutagen import File
from typing import Dict, Optional

def extract_metadata(file_path: str) -> Dict[str, Optional[str]]:
    """Extract metadata from audio file.

    Args:
        file_path: Path to the audio file

    Returns:
        Dictionary of metadata key-value pairs

    Raises:
        InvalidAudioFileError: If file cannot be read
    """
    try:
        audio_file = File(file_path)
        if audio_file is None:
            raise InvalidAudioFileError(f"Cannot read file: {file_path}")

        metadata = {
            "title": audio_file.get("title", [None])[0],
            "artist": audio_file.get("artist", [None])[0],
            "album": audio_file.get("album", [None])[0],
            "duration": str(audio_file.info.length) if hasattr(audio_file.info, 'length') else None,
            "bitrate": str(audio_file.info.bitrate) if hasattr(audio_file.info, 'bitrate') else None,
            "sample_rate": str(audio_file.info.sample_rate) if hasattr(audio_file.info, 'sample_rate') else None,
        }
        return metadata
    except Exception as e:
        raise MetadataExtractionError(f"Failed to extract metadata: {e}")
```
