# Story 6.1: Import Tracklist from 1001tracklists

## Story Information
- **Epic**: 6 - Tracklist Management
- **Story Number**: 6.1
- **Status**: Draft
- **Created**: 2025-08-27
- **Updated**: 2025-08-27
- **Dependencies**: Epic 1 (Foundation), Epic 2 (Metadata), Epic 4 (1001tracklists API), Epic 5 (CUE Handler)

## Story Statement
**As a** DJ with recorded mixes,
**I want** to import tracklists from 1001tracklists,
**So that** I can generate CUE files for my recordings.

## Acceptance Criteria
1. Search and retrieve tracklists via API
2. Match tracklist to local audio file
3. Import all track information
4. Handle timing adjustments
5. Generate CUE file automatically

## Dev Notes

### Previous Story Insights
From Story 5.5 (CUE Converter) implementation:
- CUE handler module is fully functional with parser, generator, validator, and converter components
- All format support is implemented (Standard, CDJ, Traktor, Serato, Rekordbox, Kodi)
- Validation system is comprehensive with configurable rules
- Type safety has been ensured with all mypy checks passing
- Integration patterns established for working with CUE files
[Source: Previous story 5.5 completion notes]

### Project Structure Requirements
The tracklist service already exists at `services/tracklist_service/` with established structure:
- API endpoints: `services/tracklist_service/src/api/`
- Models: `services/tracklist_service/src/models/`
- Existing API files: `tracklist_api.py`, `search_api.py`, `batch_endpoints.py`
- Main service entry: `services/tracklist_service/src/main.py`
[Source: architecture/source-tree.md]

### Data Models
**Tracklist Model** (from Epic 6):
```python
class Tracklist:
    id: UUID
    audio_file_id: UUID  # Links to Recording
    source: str  # "manual", "1001tracklists", "auto-detected"
    created_at: datetime
    updated_at: datetime
    tracks: List[TrackEntry]
    cue_file_id: Optional[UUID]
    confidence_score: float

class TrackEntry:
    position: int
    start_time: timedelta
    end_time: Optional[timedelta]
    artist: str
    title: str
    remix: Optional[str]
    label: Optional[str]
    catalog_track_id: Optional[UUID]  # Link to catalog
    confidence: float
    transition_type: Optional[str]
```
[Source: docs/prd/epic-6-tracklist-management.md#Technical Scope]

### Database Schema
**PostgreSQL Tables** (existing):
```sql
CREATE TABLE tracklists (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    recording_id UUID REFERENCES recordings(id),
    source VARCHAR(255) NOT NULL,
    cue_file_path TEXT,
    tracks JSONB
);
```
Note: The existing schema will need extension to include additional fields from the data model (confidence_score, created_at, updated_at).
[Source: architecture/database-schema-refined-and-finalized.md]

### API Specifications
**REST Endpoints to Implement**:
```
# Import Operations
POST /api/v1/tracklists/import/1001tracklists
  - Body: { url: "...", audio_file_id: "..." }
  - Returns: Imported tracklist with generated CUE file

# Search Operations (leverage existing search_api.py)
GET /api/v1/tracklists/search/1001tracklists
  - Query params: query, artist, title
  - Returns: List of matching tracklists from 1001tracklists
```
[Source: docs/prd/epic-6-tracklist-management.md#API Specification]

### Integration Requirements
**1001tracklists API Integration**:
- Must use the API client from Epic 4 (should be available in shared modules or as a service)
- Handle rate limiting and API errors gracefully
- Cache responses using Redis to avoid duplicate API calls
[Source: docs/prd/epic-6-tracklist-management.md#Integration Architecture]

**CUE File Generation**:
- Use the CUE handler from Epic 5 (`services/analysis_service/src/cue_handler/`)
- Generate CUE files automatically after successful tracklist import
- Support multiple CUE formats based on user preferences
[Source: docs/prd/epic-6-tracklist-management.md#Technical Scope]

### Service Communication
- Services communicate via RabbitMQ messages (no direct HTTP calls between services)
- Use async processing for long-running operations
- Redis for caching API responses and processing state
[Source: architecture/high-level-architecture.md, architecture/coding-standards.md#Critical Rules]

### File Locations
Based on project structure, new code should be created in:
- API endpoints: `services/tracklist_service/src/api/import_endpoints.py` (new file)
- Models updates: `services/tracklist_service/src/models/tracklist.py` (update existing)
- Import service: `services/tracklist_service/src/services/import_service.py` (new file)
- Tests: `tests/unit/tracklist_service/test_import_endpoints.py` (new file)

### Technical Constraints
- Python 3.13 with all commands using `uv run`
- SQLAlchemy for ORM operations
- Alembic for database migrations
- Type hints required with mypy validation
- Pre-commit hooks must pass before committing
[Source: architecture/tech-stack.md, architecture/coding-standards.md]

## Tasks / Subtasks

- [ ] **Task 1: Update Tracklist Data Models** (AC: 3)
  - [ ] Update `services/tracklist_service/src/models/tracklist.py` with complete Tracklist and TrackEntry models
  - [ ] Add SQLAlchemy mappings for new fields (confidence_score, created_at, updated_at)
  - [ ] Create Alembic migration to update database schema
  - [ ] Run migration with `uv run alembic upgrade head`
  - [ ] Write unit tests for model validation in `tests/unit/tracklist_service/test_models.py`

- [ ] **Task 2: Create 1001tracklists Import Service** (AC: 1, 3)
  - [ ] Create `services/tracklist_service/src/services/import_service.py`
  - [ ] Implement method to fetch tracklist data from 1001tracklists API (use Epic 4 client)
  - [ ] Implement method to parse and transform API response to TrackEntry objects
  - [ ] Add Redis caching for API responses to avoid duplicate calls
  - [ ] Write comprehensive unit tests for import service

- [ ] **Task 3: Implement Tracklist Matching Logic** (AC: 2)
  - [ ] Create matching algorithm to correlate 1001tracklists data with local audio files
  - [ ] Use audio file metadata (duration, title, artist) for matching confidence scoring
  - [ ] Implement fuzzy matching for title/artist variations
  - [ ] Add validation to ensure audio file exists and is accessible
  - [ ] Write unit tests for matching logic

- [ ] **Task 4: Implement Timing Adjustment Handler** (AC: 4)
  - [ ] Create timing adjustment logic to align track timestamps with actual audio
  - [ ] Handle different timing formats from 1001tracklists
  - [ ] Implement offset calculation for mix start times
  - [ ] Add validation for timing consistency (no overlaps, within audio duration)
  - [ ] Write unit tests for timing adjustments

- [ ] **Task 5: Integrate CUE File Generation** (AC: 5)
  - [ ] Import CUE handler from `services/analysis_service/src/cue_handler/`
  - [ ] Create method to convert Tracklist to CUE format
  - [ ] Generate CUE file after successful import
  - [ ] Store CUE file path in database
  - [ ] Write integration tests for CUE generation

- [ ] **Task 6: Create Import API Endpoints** (AC: 1, 5)
  - [ ] Create `services/tracklist_service/src/api/import_endpoints.py`
  - [ ] Implement `POST /api/v1/tracklists/import/1001tracklists` endpoint
  - [ ] Add request validation and error handling
  - [ ] Implement async processing for long-running imports
  - [ ] Return appropriate status codes and response format
  - [ ] Write API tests using pytest

- [ ] **Task 7: Add Search Endpoint for 1001tracklists** (AC: 1)
  - [ ] Extend existing `services/tracklist_service/src/api/search_api.py`
  - [ ] Implement `GET /api/v1/tracklists/search/1001tracklists` endpoint
  - [ ] Add query parameter validation
  - [ ] Implement result pagination
  - [ ] Write API tests for search functionality

- [ ] **Task 8: Implement Error Handling and Logging**
  - [ ] Add comprehensive error handling for API failures
  - [ ] Implement retry logic with exponential backoff
  - [ ] Add structured logging for debugging
  - [ ] Create custom exceptions for import-specific errors
  - [ ] Write tests for error scenarios

- [ ] **Task 9: Add RabbitMQ Message Handler**
  - [ ] Create message handler for async import processing
  - [ ] Define message schema for import requests
  - [ ] Implement message publishing from API endpoint
  - [ ] Add message consumer in import service
  - [ ] Write integration tests for message flow

- [ ] **Task 10: Run All Tests and Validation**
  - [ ] Run all unit tests: `uv run pytest tests/unit/tracklist_service/ -v`
  - [ ] Run integration tests if services available
  - [ ] Run pre-commit hooks: `uv run pre-commit run --all-files`
  - [ ] Ensure mypy type checking passes: `uv run mypy`
  - [ ] Verify code coverage meets 80% threshold

## Testing Standards
[Source: architecture/test-strategy-and-standards.md]

- **Framework**: Use `pytest` with `uv run pytest` execution
- **Test Files**: Place in `tests/unit/tracklist_service/` with `test_*.py` naming
- **Coverage Goal**: Minimum 80% for new code
- **Test Types**: Write unit tests for all public methods, cover edge cases
- **Pre-commit**: Must pass all hooks before committing (`uv run pre-commit run --all-files`)
- **Validation**: Tests must pass before story can be marked complete

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-27 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
(To be populated by Dev Agent)

### Debug Log References
(To be populated by Dev Agent)

### Completion Notes
(To be populated by Dev Agent)

### File List
(To be populated by Dev Agent)

## QA Results
(To be populated by QA Agent)
